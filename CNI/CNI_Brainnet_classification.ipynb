{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNI_Brainnet_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meIUtJYU3IPB",
        "outputId": "34b356ed-c4e3-44cf-f067-ddf1d88f1eb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) \n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXq4djoM3KBs",
        "outputId": "f8105d19-28dc-4516-c979-77a20e26e1e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 8.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 8.1 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "label = pd.read_csv(\"phenotypic_training.csv\")\n",
        "labellist = []\n",
        "for i in label[\"DX\"]:\n",
        "    if i == \"ADHD\":\n",
        "        labellist.append([0])\n",
        "    else:\n",
        "        labellist.append([1])\n",
        "labellist=torch.tensor(labellist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiGUZ_F23LKf",
        "outputId": "04873e9a-36c6-410f-e511-cbc75eaa0ae3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "graphlist=[]\n",
        "import networkx as nx\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Dataset\n",
        "#from torch_geometric.loader import DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "%cd /content/drive/MyDrive/Training\n",
        "for i in range(1, 600):\n",
        "  if os.path.isfile(\"sub-\"+ \"{0:0>3}\".format(i)+\"/timeseries_aal.csv\"):\n",
        "    tmp = pd.read_csv(\"sub-\"+ \"{0:0>3}\".format(i)+\"/timeseries_aal.csv\", header = None)\n",
        "    tmp = tmp.T\n",
        "    tmp = tmp.corr(method=\"pearson\")\n",
        "    tmp=(tmp.to_numpy())\n",
        "    tmp=torch.from_numpy(tmp) \n",
        "    tmp=torch.div(tmp,tmp.max())\n",
        "    #graphlist.append(data)\n",
        "    graphlist.append(tmp.type(torch.FloatTensor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PyFScpd3K6L",
        "outputId": "bd6ae68c-3f93-4dd7-a060-84e6df88ce5d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "for i in range(len(graphlist)):\n",
        "  dataset.append([graphlist[i], labellist[i]])"
      ],
      "metadata": {
        "id": "hWd9XUnx3Kxd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset = random.sample(dataset, len(dataset))\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=False)"
      ],
      "metadata": {
        "id": "MOzxK7IU3KqO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class E2EBlock(torch.nn.Module):\n",
        "    def __init__(self, in_planes, planes,bias=True):\n",
        "        super(E2EBlock, self).__init__()\n",
        "        self.d = 116\n",
        "        self.cnn1 = torch.nn.Conv2d(in_planes,planes,(1,self.d),bias=bias)\n",
        "        self.cnn2 = torch.nn.Conv2d(in_planes,planes,(self.d,1),bias=bias)       \n",
        "    def forward(self, x):\n",
        "        a = self.cnn1(x)\n",
        "        b = self.cnn2(x)\n",
        "        return torch.cat([a]*self.d,3)+torch.cat([b]*self.d,2)\n",
        "\n",
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.d = 116\n",
        "        self.e2econv1 = E2EBlock(1,32)   ######### Changed input channel from 1 to 3\n",
        "        self.e2econv2 = E2EBlock(32,64)\n",
        "        self.E2N = torch.nn.Conv2d(64,1,(1,self.d))\n",
        "        self.N2G = torch.nn.Conv2d(1,256,(self.d,1))\n",
        "        self.dense1 = torch.nn.Linear(256,128)\n",
        "        self.dense2 = torch.nn.Linear(128,30)\n",
        "        self.aux = torch.nn.Linear(30,1)\n",
        "      \n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(self.e2econv1(x),negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.e2econv2(out),negative_slope=0.33) \n",
        "        out = F.leaky_relu(self.E2N(out),negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.N2G(out),negative_slope=0.33)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.leaky_relu(self.dense1(out),negative_slope=0.33)\n",
        "        out = F.leaky_relu(self.dense2(out),negative_slope=0.33)\n",
        "        classes = self.aux(out) \n",
        "        return classes"
      ],
      "metadata": {
        "id": "NjPt8Df83Kdx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "sig=torch.nn.Sigmoid()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Classifier().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "correct = 0\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data, label in loader:  # Iterate in batches over the training/test dataset.\n",
        "         label_classes = label.type(torch.float).cuda() \n",
        "         cur_batch_size = torch.tensor(len(data)).to(device)\n",
        "         data = data.view(cur_batch_size, -1).to(device)\n",
        "         data = data.view(cur_batch_size, 1, 116, 116)\n",
        "         out = model(data.to(device))  \n",
        "         pred = sig(out)>0.5  # Use the class with highest probability.\n",
        "         #print(out.size())\n",
        "         #print(int((pred == data.y).sum()))\n",
        "         #print(label_classes.size(), \"label\")\n",
        "         #print(pred.size(), \"pred\")\n",
        "         label_classes = label_classes.view(-1)\n",
        "         correct += int((pred.view(-1) == label_classes).sum())\n",
        "         #print(correct)  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1,100):\n",
        "    model.train()\n",
        "\n",
        "    for data, label in train_loader:  # Iterate in batches over the training dataset.\n",
        "         label_classes = label.type(torch.float).cuda() \n",
        "         cur_batch_size = torch.tensor(len(data)).to(device)\n",
        "         data = data.view(cur_batch_size, -1).to(device)\n",
        "         data = data.view(cur_batch_size, 1, 116, 116)\n",
        "         out = model(data.cuda())\n",
        "         out = out.view(-1)\n",
        "         label = label.view(-1).cuda()\n",
        "         #print(label.size(), \"label\")\n",
        "         loss = criterion(out, label.float())  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, Loss: {loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "w-bFB4kA3KV9",
        "outputId": "fe433a77-82ef-48fa-bede-b1b2646fd649"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.5133, Test Acc: 0.4600, Loss: 0.7008\n",
            "Epoch: 002, Train Acc: 0.5600, Test Acc: 0.5000, Loss: 0.7032\n",
            "Epoch: 003, Train Acc: 0.5133, Test Acc: 0.4600, Loss: 0.6923\n",
            "Epoch: 004, Train Acc: 0.4800, Test Acc: 0.5400, Loss: 0.6972\n",
            "Epoch: 005, Train Acc: 0.5133, Test Acc: 0.4600, Loss: 0.6916\n",
            "Epoch: 006, Train Acc: 0.5133, Test Acc: 0.4600, Loss: 0.6904\n",
            "Epoch: 007, Train Acc: 0.5400, Test Acc: 0.4600, Loss: 0.6945\n",
            "Epoch: 008, Train Acc: 0.5133, Test Acc: 0.5600, Loss: 0.7169\n",
            "Epoch: 009, Train Acc: 0.4933, Test Acc: 0.5800, Loss: 0.7167\n",
            "Epoch: 010, Train Acc: 0.5267, Test Acc: 0.5000, Loss: 0.6932\n",
            "Epoch: 011, Train Acc: 0.7133, Test Acc: 0.4800, Loss: 0.6903\n",
            "Epoch: 012, Train Acc: 0.6067, Test Acc: 0.4600, Loss: 0.7015\n",
            "Epoch: 013, Train Acc: 0.6733, Test Acc: 0.5000, Loss: 0.6784\n",
            "Epoch: 014, Train Acc: 0.7267, Test Acc: 0.4800, Loss: 0.4860\n",
            "Epoch: 015, Train Acc: 0.6867, Test Acc: 0.5200, Loss: 0.4434\n",
            "Epoch: 016, Train Acc: 0.8867, Test Acc: 0.5400, Loss: 0.2748\n",
            "Epoch: 017, Train Acc: 0.9800, Test Acc: 0.7000, Loss: 0.2575\n",
            "Epoch: 018, Train Acc: 0.5800, Test Acc: 0.5200, Loss: 0.0562\n",
            "Epoch: 019, Train Acc: 0.6600, Test Acc: 0.5800, Loss: 0.3161\n",
            "Epoch: 020, Train Acc: 0.9533, Test Acc: 0.5600, Loss: 0.3123\n",
            "Epoch: 021, Train Acc: 0.9267, Test Acc: 0.5800, Loss: 0.1558\n",
            "Epoch: 022, Train Acc: 0.9067, Test Acc: 0.5200, Loss: 0.2286\n",
            "Epoch: 023, Train Acc: 0.9667, Test Acc: 0.6400, Loss: 0.0638\n",
            "Epoch: 024, Train Acc: 0.9933, Test Acc: 0.6800, Loss: 0.0237\n",
            "Epoch: 025, Train Acc: 1.0000, Test Acc: 0.5800, Loss: 0.0586\n",
            "Epoch: 026, Train Acc: 1.0000, Test Acc: 0.5800, Loss: 0.0007\n",
            "Epoch: 027, Train Acc: 1.0000, Test Acc: 0.5200, Loss: 0.0017\n",
            "Epoch: 028, Train Acc: 1.0000, Test Acc: 0.5200, Loss: 0.0006\n",
            "Epoch: 029, Train Acc: 1.0000, Test Acc: 0.5600, Loss: 0.0003\n",
            "Epoch: 030, Train Acc: 1.0000, Test Acc: 0.6000, Loss: 0.0000\n",
            "Epoch: 031, Train Acc: 1.0000, Test Acc: 0.5600, Loss: 0.0001\n",
            "Epoch: 032, Train Acc: 1.0000, Test Acc: 0.5600, Loss: 0.0000\n",
            "Epoch: 033, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0001\n",
            "Epoch: 034, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0001\n",
            "Epoch: 035, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 036, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 037, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 038, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 039, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 040, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 041, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 042, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 043, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 044, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 045, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 046, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 047, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 048, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 049, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 050, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 051, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 052, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 053, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 054, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 055, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 056, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 057, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 058, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 059, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 060, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 061, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 062, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 063, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 064, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 065, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 066, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 067, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 068, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 069, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 070, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 071, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 072, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 073, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 074, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 075, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 076, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 077, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 078, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 079, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 080, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 081, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 082, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 083, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 084, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 085, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 086, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 087, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 088, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 089, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 090, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 091, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 092, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 093, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 094, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 095, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 096, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 097, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 098, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n",
            "Epoch: 099, Train Acc: 1.0000, Test Acc: 0.5400, Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xhz88GBI43zz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}