{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0tzsa31hdNv",
        "outputId": "92719450-5fb5-48d7-eee7-28ccd77fc5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) \n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L9of6xlhjiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5e98ba-d96a-4a3e-96ff-7e70b62c3e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 45.4 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kpikxgQhk9b",
        "outputId": "c65cc1e8-ed8b-49e8-8d08-f1dd630e07e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10M6M6of9sCS0_CWq5d9EF6LhBCK9klNf/ALL_NETWORKS\n"
          ]
        }
      ],
      "source": [
        "train_dataset=[]\n",
        "graphlist=[]\n",
        "import networkx as nx\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "#from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "#%cd /content/drive/MyDrive/GNN_DATA/ADNI/ALL_NETWORKS\n",
        "%cd /content/drive/MyDrive/ALL_NETWORKS\n",
        "\n",
        "for i in range(1, 110):\n",
        "    if os.path.isfile('AD_'+'{0:0>3}'.format(i)+'.csv'):\n",
        "      tmp=pd.read_csv('AD_'+'{0:0>3}'.format(i)+'.csv', header=None)\n",
        "      tmp=(tmp.to_numpy())\n",
        "      tmp=torch.from_numpy(tmp)  \n",
        "      tmp=torch.div(tmp,tmp.max())\n",
        "      #print([tmp.type(torch.FloatTensor),torch.ones(1).type(torch.long)])\n",
        "      G = nx.from_numpy_matrix(np.array(tmp))\n",
        "      data = from_networkx(G)\n",
        "      deg = degree(data.edge_index[0], data.num_nodes)\n",
        "      x = []\n",
        "      for i in deg:\n",
        "        x.extend([i])\n",
        "      x = np.array(x).reshape(90,1)\n",
        "      data = from_networkx(G)\n",
        "      #x = torch.tensor([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1] ], dtype=torch.float)\n",
        "      data.x = torch.FloatTensor(x)\n",
        "      data.y = torch.tensor([1])\n",
        "      graphlist.append(data)\n",
        "      #graphlist.append([tmp.type(torch.FloatTensor),torch.ones(1).type(torch.long)])\n",
        "\n",
        "for i in range(1, 110):\n",
        "    if os.path.isfile('CN_'+'{0:0>3}'.format(i)+'.csv'):\n",
        "      tmp=pd.read_csv('CN_'+'{0:0>3}'.format(i)+'.csv', header=None)\n",
        "      tmp=(tmp.to_numpy())\n",
        "      tmp=torch.from_numpy(tmp) \n",
        "      tmp=torch.div(tmp, tmp.max())\n",
        "      #print([tmp.type(torch.FloatTensor),torch.ones(1).type(torch.long)])\n",
        "      G = nx.from_numpy_matrix(np.array(tmp))\n",
        "      data = from_networkx(G)\n",
        "      deg = degree(data.edge_index[0], data.num_nodes)\n",
        "      x = []\n",
        "      for i in deg:\n",
        "        x.extend([i])\n",
        "      x = np.array(x).reshape(90,1)\n",
        "      data = from_networkx(G)\n",
        "      #x = torch.tensor([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1] ], dtype=torch.float)\n",
        "      data.x = torch.FloatTensor(x)\n",
        "      data.y = torch.tensor([0])\n",
        "      graphlist.append(data)\n",
        "      #graphlist.append([data, torch.zeros(1).type(torch.long)])\n",
        "\n",
        "      #graphlist.append([tmp.type(torch.FloatTensor),torch.zeros(1).type(torch.long)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8hqHqVMhvdM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30a4ff1-f1fd-4dbf-f5f3-cc5037153f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive\n",
        "pos = pd.read_csv(\"node_aal90.csv\", header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCmWWBI7hxzD"
      },
      "outputs": [],
      "source": [
        "position1 = []\n",
        "for i in pos[0]:\n",
        "  position1.extend([i])\n",
        "position1 = np.array(position1).reshape(90,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WV8WnZhKhzdD"
      },
      "outputs": [],
      "source": [
        "position2 = []\n",
        "for i in pos[1]:\n",
        "  position2.extend([i])\n",
        "position2 = np.array(position2).reshape(90,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpxuYY7Fh00r"
      },
      "outputs": [],
      "source": [
        "position3 = []\n",
        "for i in pos[2]:\n",
        "  position3.extend([i])\n",
        "position3 = np.array(position3).reshape(90,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsPrJrpPh2Ir"
      },
      "outputs": [],
      "source": [
        "position = np.concatenate((position1, position2, position3), axis = 1) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position"
      ],
      "metadata": {
        "id": "nlfE_1wp-7Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hFnSJE4h4dL"
      },
      "outputs": [],
      "source": [
        "for i in graphlist:\n",
        "  i.pos = torch.FloatTensor(position)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m14uZc3Mh7XL"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "dataset = random.sample(graphlist, len(graphlist))\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0].edge_index"
      ],
      "metadata": {
        "id": "HJ_5NUcpxXFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36748b68-3b12-406f-b40b-d77ab03ee398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0,  ..., 89, 89, 89],\n",
              "        [ 1,  2,  3,  ..., 83, 85, 87]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.utils import normalized_cut\n",
        "\n",
        "\n",
        "def normalized_cut_2d(edge_index, pos):\n",
        "    row, col = edge_index\n",
        "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
        "    #print(edge_attr)\n",
        "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))\n",
        "\n",
        "def normalized_cut_3d(edge_index, pos):\n",
        "    row, col= edge_index\n",
        "    print(pos[col], \"row\")\n",
        "    edge_attr = torch.norm(pos[row] - pos[col], p=3, dim = 1)\n",
        "    print(edge_attr)\n",
        "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0)) \n",
        "\n",
        "stmp = normalized_cut_3d(dataset[0].edge_index, dataset[0].pos)"
      ],
      "metadata": {
        "id": "FPDF_3Gjxbr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "942db508-a130-4769-e0c0-aa4281e9e0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 41.3700,  -8.2100,  52.0900],\n",
            "        [-18.4500,  34.8100,  42.2000],\n",
            "        [ 21.9000,  31.1200,  43.8200],\n",
            "        ...,\n",
            "        [ 48.2500,  14.7500, -16.8600],\n",
            "        [ 57.4700, -37.2300,  -1.4700],\n",
            "        [ 44.2200,  14.5500, -32.2300]]) row\n",
            "tensor([80.0209, 42.2261, 64.8075,  ..., 45.8713, 21.0683, 45.9101])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjb6Usv8h9Lr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.utils import normalized_cut\n",
        "\n",
        "\n",
        "def normalized_cut_2d(edge_index, pos):\n",
        "    row, col = edge_index\n",
        "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
        "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KybAW82h-u7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops\n",
        "\n",
        "\n",
        "class SpatialGraphConv(MessagePassing):\n",
        "    def __init__(self, coors, in_channels, out_channels, hidden_size, dropout=0):\n",
        "        \"\"\"\n",
        "        coors - dimension of positional descriptors (e.g. 2 for 2D images)\n",
        "        in_channels - number of the input channels (node features)\n",
        "        out_channels - number of the output channels (node features)\n",
        "        hidden_size - number of the inner convolutions\n",
        "        dropout - dropout rate after the layer\n",
        "        \"\"\"\n",
        "        super(SpatialGraphConv, self).__init__(aggr='add')\n",
        "        self.dropout = dropout\n",
        "        self.lin_in = torch.nn.Linear(coors, hidden_size * in_channels)\n",
        "        self.lin_out = torch.nn.Linear(hidden_size * in_channels, out_channels)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "    def forward(self, x, pos, edge_index):\n",
        "        \"\"\"\n",
        "        x - feature matrix of the whole graph [num_nodes, label_dim]\n",
        "        pos - node position matrix [num_nodes, coors]\n",
        "        edge_index - graph connectivity [2, num_edges]\n",
        "        \"\"\"\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))  # num_edges = num_edges + num_nodes\n",
        "\n",
        "        return self.propagate(edge_index=edge_index, x=x, pos=pos, aggr='add')  # [N, out_channels, label_dim]\n",
        "\n",
        "    def message(self, pos_i, pos_j, x_j):\n",
        "        \"\"\"\n",
        "        pos_i [num_edges, coors]\n",
        "        pos_j [num_edges, coors]\n",
        "        x_j [num_edges, label_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        relative_pos = pos_j - pos_i  # [n_edges, hidden_size * in_channels]\n",
        "        spatial_scaling = F.relu(self.lin_in(relative_pos))  # [n_edges, hidden_size * in_channels]\n",
        "\n",
        "        n_edges = spatial_scaling.size(0)\n",
        "        # [n_edges, in_channels, ...] * [n_edges, in_channels, 1]\n",
        "        result = spatial_scaling.reshape(n_edges, self.in_channels, -1) * x_j.unsqueeze(-1)\n",
        "        return result.view(n_edges, -1)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        \"\"\"\n",
        "        aggr_out [num_nodes, label_dim, out_channels]\n",
        "        \"\"\"\n",
        "        aggr_out = self.lin_out(aggr_out)  # [num_nodes, label_dim, out_features]\n",
        "        aggr_out = F.relu(aggr_out)\n",
        "        aggr_out = F.dropout(aggr_out, p=self.dropout, training=self.training)\n",
        "\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJSPbxG9iAJz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import graclus, max_pool, global_mean_pool\n",
        "\n",
        "class SGCN(torch.nn.Module):\n",
        "    def __init__(self, dim_coor, out_dim, input_features,\n",
        "                 layers_num, model_dim, out_channels_1, dropout,\n",
        "                 use_cluster_pooling):\n",
        "        super(SGCN, self).__init__()\n",
        "        self.layers_num = layers_num\n",
        "        self.use_cluster_pooling = use_cluster_pooling\n",
        "\n",
        "        self.conv_layers = [SpatialGraphConv(coors=dim_coor,\n",
        "                                             in_channels=input_features,\n",
        "                                             out_channels=model_dim,\n",
        "                                             hidden_size=out_channels_1,\n",
        "                                             dropout=dropout)] + \\\n",
        "                           [SpatialGraphConv(coors=dim_coor,\n",
        "                                             in_channels=model_dim,\n",
        "                                             out_channels=model_dim,\n",
        "                                             hidden_size=out_channels_1,\n",
        "                                             dropout=dropout) for _ in range(layers_num - 1)]\n",
        "\n",
        "        self.conv_layers = torch.nn.ModuleList(self.conv_layers)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(model_dim, out_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        for i in range(self.layers_num):\n",
        "            data.x = self.conv_layers[i](data.x, data.pos, data.edge_index)\n",
        "\n",
        "            if self.use_cluster_pooling:\n",
        "                weight = normalized_cut_2d(data.edge_index, data.pos)\n",
        "                cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
        "                data = max_pool(cluster, data, transform=T.Cartesian(cat=False))\n",
        "\n",
        "        data.x = global_mean_pool(data.x, data.batch)\n",
        "        x = self.fc1(data.x)\n",
        "\n",
        "        return x\n",
        "        #return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2MSYrZ-iB3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e057bd-4240-48af-e07d-2b4513f270f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 12489700.66667, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 002, Loss: 310118.78385, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 003, Loss: 55181.47426, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 004, Loss: 2035.26542, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 005, Loss: 0.70037, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 006, Loss: 0.70030, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 007, Loss: 0.70001, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 008, Loss: 0.69981, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 009, Loss: 0.69958, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 010, Loss: 0.69933, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 011, Loss: 0.69910, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 012, Loss: 0.69881, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 013, Loss: 0.69860, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 014, Loss: 0.69834, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 015, Loss: 0.69807, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 016, Loss: 0.69777, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 017, Loss: 0.69765, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 018, Loss: 0.69735, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 019, Loss: 0.69708, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 020, Loss: 0.69677, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 021, Loss: 0.69665, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 022, Loss: 0.69617, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 023, Loss: 0.69612, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 024, Loss: 0.69577, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 025, Loss: 0.69561, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 026, Loss: 0.69534, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 027, Loss: 0.69515, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 028, Loss: 0.69489, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 029, Loss: 0.69471, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 030, Loss: 0.69462, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 031, Loss: 0.69448, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 032, Loss: 0.69403, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 033, Loss: 0.69399, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 034, Loss: 0.69377, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 035, Loss: 0.69366, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 036, Loss: 0.69334, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 037, Loss: 0.69316, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 038, Loss: 0.69313, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 039, Loss: 0.69302, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 040, Loss: 0.69260, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 041, Loss: 0.69259, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 042, Loss: 0.69268, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 043, Loss: 0.69216, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 044, Loss: 0.69201, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 045, Loss: 0.69194, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 046, Loss: 0.69196, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 047, Loss: 0.69182, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 048, Loss: 0.69153, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 049, Loss: 0.69169, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 050, Loss: 0.69158, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 051, Loss: 0.69145, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 052, Loss: 0.69107, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 053, Loss: 0.69085, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 054, Loss: 0.69072, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 055, Loss: 0.69077, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 056, Loss: 0.69081, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 057, Loss: 0.69103, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 058, Loss: 0.69046, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 059, Loss: 0.69048, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 060, Loss: 0.69047, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 061, Loss: 0.68999, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 062, Loss: 0.69017, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 063, Loss: 0.69006, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 064, Loss: 0.68997, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 065, Loss: 0.69007, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 066, Loss: 0.68949, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 067, Loss: 0.68980, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 068, Loss: 0.68969, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 069, Loss: 0.68930, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 070, Loss: 0.68951, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 071, Loss: 0.68934, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 072, Loss: 0.68932, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 073, Loss: 0.68905, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 074, Loss: 0.68935, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 075, Loss: 0.68896, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 076, Loss: 0.68910, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 077, Loss: 0.68908, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 078, Loss: 0.68910, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 079, Loss: 0.68876, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 080, Loss: 0.68861, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 081, Loss: 0.68877, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 082, Loss: 0.68911, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 083, Loss: 0.68895, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 084, Loss: 0.68836, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 085, Loss: 0.68847, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 086, Loss: 0.68879, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 087, Loss: 0.68865, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 088, Loss: 0.68867, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 089, Loss: 0.68837, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 090, Loss: 0.68824, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 091, Loss: 0.68853, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 092, Loss: 0.68839, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 093, Loss: 0.68846, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 094, Loss: 0.68858, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 095, Loss: 0.68851, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 096, Loss: 0.68816, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 097, Loss: 0.68778, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 098, Loss: 0.68830, Train Acc: 0.55333, Test Acc: 0.40385\n",
            "Epoch: 099, Loss: 0.68813, Train Acc: 0.55333, Test Acc: 0.40385\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sig=torch.nn.Sigmoid()\n",
        "model = SGCN(dim_coor=3,\n",
        "             out_dim=1,\n",
        "             input_features=1,\n",
        "             layers_num=3,\n",
        "             model_dim=16,\n",
        "             out_channels_1=64,\n",
        "             dropout=0.3,\n",
        "             use_cluster_pooling=True).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "#rotation_0 = T.RandomRotate(degrees=180, axis=0)\n",
        "#rotation_1 = T.RandomRotate(degrees=180, axis=1)\n",
        "#rotation_2 = T.RandomRotate(degrees=180, axis=2)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "\n",
        "    loss_all = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output.view(-1)\n",
        "        loss = criterion(output, data.y.float()) \n",
        "        #loss = F.nll_loss(output, data.y)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        out = model(data).max(dim=1)[1]\n",
        "        pred = sig(out)>0.5\n",
        "        #pred = model(data).max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "train_acc_array = []\n",
        "test_acc_array = []\n",
        "\n",
        "for epoch in range(1, 100):\n",
        "    loss = train(epoch)\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "\n",
        "    train_acc_array.append(train_acc)\n",
        "    test_acc_array.append(test_acc)\n",
        "\n",
        "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.format(epoch, loss, train_acc, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0iJt8PdtayGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AD_SCGN_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}