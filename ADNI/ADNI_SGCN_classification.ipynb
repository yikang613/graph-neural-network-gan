{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADNI_SGCN_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuFo7ACEGFYi",
        "outputId": "fbe9e151-fa65-47cf-ecc9-0816d8836f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0) \n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "#!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "id": "Ffo_HmT8GHCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a63903-7613-42ab-ebd4-dac3c387c064"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 6.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 7.4 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import MNISTSuperpixels"
      ],
      "metadata": {
        "id": "8m10HxG-GIfB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = os.path.join(os.path.dirname(os.getcwd()), 'data', 'MNIST')\n",
        "train_dataset = MNISTSuperpixels(path, True, transform=T.Cartesian())\n",
        "test_dataset = MNISTSuperpixels(path, False, transform=T.Cartesian())\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n"
      ],
      "metadata": {
        "id": "rlQzglWSGSqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a38e8d-0370-4f61-938a-e59e50ea66dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://data.pyg.org/datasets/MNISTSuperpixels.zip\n",
            "Extracting /data/MNIST/raw/MNISTSuperpixels.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = train_dataset[20]"
      ],
      "metadata": {
        "id": "VaFfrWgbGa4J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5dPLR52GnZQ",
        "outputId": "80b72b5f-7470-420b-f18e-c39968e5f19a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive\n",
        "pos = pd.read_csv(\"node_aal90.csv\", header = None)"
      ],
      "metadata": {
        "id": "NRJv0CVJJL_S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20254597-587d-41d6-da63-21b8504e1a20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position1 = []\n",
        "for i in pos[0]:\n",
        "  position1.extend([i])\n",
        "position1 = np.array(position1).reshape(90,1)"
      ],
      "metadata": {
        "id": "Rv2z9f7iHyt6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position2 = []\n",
        "for i in pos[1]:\n",
        "  position2.extend([i])\n",
        "position2 = np.array(position2).reshape(90,1)"
      ],
      "metadata": {
        "id": "99NlYSqAIGiF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position3 = []\n",
        "for i in pos[2]:\n",
        "  position3.extend([i])\n",
        "position3 = np.array(position3).reshape(90,1)"
      ],
      "metadata": {
        "id": "i9BlfUA9JL4c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(position3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXfzbYGiJQas",
        "outputId": "ac2e77df-4cdb-4f10-ee36-132279b8c93e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = np.concatenate((position1, position2, position3), axis = 1) "
      ],
      "metadata": {
        "id": "dPzMZUTnJS-F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(position)"
      ],
      "metadata": {
        "id": "NRy6dCcNJkc1",
        "outputId": "ca03dae0-20ec-4633-e0fa-8299b97aaf1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position"
      ],
      "metadata": {
        "id": "yFC6SR5dJl-0",
        "outputId": "9d557afa-a37a-4ac5-9670-92733431ef40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.865e+01, -5.680e+00,  5.094e+01],\n",
              "       [ 4.137e+01, -8.210e+00,  5.209e+01],\n",
              "       [-1.845e+01,  3.481e+01,  4.220e+01],\n",
              "       [ 2.190e+01,  3.112e+01,  4.382e+01],\n",
              "       [-1.656e+01,  4.732e+01, -1.331e+01],\n",
              "       [ 1.849e+01,  4.810e+01, -1.402e+01],\n",
              "       [-3.343e+01,  3.273e+01,  3.546e+01],\n",
              "       [ 3.759e+01,  3.306e+01,  3.404e+01],\n",
              "       [-3.065e+01,  5.043e+01, -9.620e+00],\n",
              "       [ 3.318e+01,  5.259e+01, -1.073e+01],\n",
              "       [-4.843e+01,  1.273e+01,  1.902e+01],\n",
              "       [ 5.020e+01,  1.498e+01,  2.141e+01],\n",
              "       [-4.558e+01,  2.991e+01,  1.399e+01],\n",
              "       [ 5.033e+01,  3.016e+01,  1.417e+01],\n",
              "       [-3.598e+01,  3.071e+01, -1.211e+01],\n",
              "       [ 4.122e+01,  3.223e+01, -1.191e+01],\n",
              "       [-4.716e+01, -8.480e+00,  1.395e+01],\n",
              "       [ 5.265e+01, -6.250e+00,  1.463e+01],\n",
              "       [-5.320e+00,  4.850e+00,  6.138e+01],\n",
              "       [ 8.620e+00,  1.700e-01,  6.185e+01],\n",
              "       [-8.060e+00,  1.505e+01, -1.146e+01],\n",
              "       [ 1.043e+01,  1.591e+01, -1.126e+01],\n",
              "       [-4.800e+00,  4.917e+01,  3.089e+01],\n",
              "       [ 9.100e+00,  5.084e+01,  3.022e+01],\n",
              "       [-5.170e+00,  5.406e+01, -7.400e+00],\n",
              "       [ 8.160e+00,  5.167e+01, -7.130e+00],\n",
              "       [-5.080e+00,  3.707e+01, -1.814e+01],\n",
              "       [ 8.350e+00,  3.564e+01, -1.804e+01],\n",
              "       [-3.513e+01,  6.650e+00,  3.440e+00],\n",
              "       [ 3.902e+01,  6.250e+00,  2.080e+00],\n",
              "       [-4.040e+00,  3.540e+01,  1.395e+01],\n",
              "       [ 8.460e+00,  3.701e+01,  1.584e+01],\n",
              "       [-5.480e+00, -1.492e+01,  4.157e+01],\n",
              "       [ 8.020e+00, -8.830e+00,  3.979e+01],\n",
              "       [-4.850e+00, -4.292e+01,  2.467e+01],\n",
              "       [ 7.440e+00, -4.181e+01,  2.187e+01],\n",
              "       [-2.503e+01, -2.074e+01, -1.013e+01],\n",
              "       [ 2.923e+01, -1.978e+01, -1.033e+01],\n",
              "       [-2.117e+01, -1.595e+01, -2.070e+01],\n",
              "       [ 2.538e+01, -1.515e+01, -2.047e+01],\n",
              "       [-2.327e+01, -6.700e-01, -1.714e+01],\n",
              "       [ 2.732e+01,  6.400e-01, -1.750e+01],\n",
              "       [-7.140e+00, -7.867e+01,  6.440e+00],\n",
              "       [ 1.599e+01, -7.315e+01,  9.400e+00],\n",
              "       [-5.930e+00, -8.013e+01,  2.722e+01],\n",
              "       [ 1.351e+01, -7.936e+01,  2.823e+01],\n",
              "       [-1.462e+01, -6.756e+01, -4.630e+00],\n",
              "       [ 1.629e+01, -6.693e+01, -3.870e+00],\n",
              "       [-1.654e+01, -8.426e+01,  2.817e+01],\n",
              "       [ 2.429e+01, -8.085e+01,  3.059e+01],\n",
              "       [-3.239e+01, -8.073e+01,  1.611e+01],\n",
              "       [ 3.739e+01, -7.970e+01,  1.942e+01],\n",
              "       [-3.636e+01, -7.829e+01, -7.840e+00],\n",
              "       [ 3.816e+01, -8.199e+01, -7.610e+00],\n",
              "       [-3.116e+01, -4.030e+01, -2.023e+01],\n",
              "       [ 3.397e+01, -3.910e+01, -2.018e+01],\n",
              "       [-4.246e+01, -2.263e+01,  4.892e+01],\n",
              "       [ 4.143e+01, -2.549e+01,  5.255e+01],\n",
              "       [-2.345e+01, -5.956e+01,  5.896e+01],\n",
              "       [ 2.611e+01, -5.918e+01,  6.206e+01],\n",
              "       [-4.280e+01, -4.582e+01,  4.674e+01],\n",
              "       [ 4.646e+01, -4.629e+01,  4.954e+01],\n",
              "       [-5.579e+01, -3.364e+01,  3.045e+01],\n",
              "       [ 5.761e+01, -3.150e+01,  3.448e+01],\n",
              "       [-4.414e+01, -6.082e+01,  3.559e+01],\n",
              "       [ 4.551e+01, -5.998e+01,  3.863e+01],\n",
              "       [-7.240e+00, -5.607e+01,  4.801e+01],\n",
              "       [ 9.980e+00, -5.605e+01,  4.377e+01],\n",
              "       [-7.630e+00, -2.536e+01,  7.007e+01],\n",
              "       [ 7.480e+00, -3.159e+01,  6.809e+01],\n",
              "       [-1.146e+01,  1.100e+01,  9.240e+00],\n",
              "       [ 1.484e+01,  1.207e+01,  9.420e+00],\n",
              "       [-2.391e+01,  3.860e+00,  2.400e+00],\n",
              "       [ 2.778e+01,  4.910e+00,  2.460e+00],\n",
              "       [-1.775e+01, -3.000e-02,  2.100e-01],\n",
              "       [ 2.120e+01,  1.800e-01,  2.300e-01],\n",
              "       [-1.085e+01, -1.756e+01,  7.980e+00],\n",
              "       [ 1.300e+01, -1.755e+01,  8.090e+00],\n",
              "       [-4.199e+01, -1.888e+01,  9.980e+00],\n",
              "       [ 4.586e+01, -1.715e+01,  1.041e+01],\n",
              "       [-5.316e+01, -2.068e+01,  7.130e+00],\n",
              "       [ 5.815e+01, -2.178e+01,  6.800e+00],\n",
              "       [-3.988e+01,  1.514e+01, -2.018e+01],\n",
              "       [ 4.825e+01,  1.475e+01, -1.686e+01],\n",
              "       [-5.552e+01, -3.380e+01, -2.200e+00],\n",
              "       [ 5.747e+01, -3.723e+01, -1.470e+00],\n",
              "       [-3.632e+01,  1.459e+01, -3.408e+01],\n",
              "       [ 4.422e+01,  1.455e+01, -3.223e+01],\n",
              "       [-4.977e+01, -2.805e+01, -2.317e+01],\n",
              "       [ 5.369e+01, -3.107e+01, -2.232e+01]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "graphlist=[]\n",
        "import networkx as nx\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "#from torch.utils.data import DataLoader\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_networkx, from_networkx\n",
        "\n",
        "\n",
        "%cd /content/drive/MyDrive/ADNI2/Controls\n",
        "for i in range(1, 110):\n",
        "    if os.path.isfile('{0:0>3}'.format(i)+'.txt'):\n",
        "      tmp=pd.read_csv('{0:0>3}'.format(i)+'.txt', header=None)\n",
        "      tmp=(tmp.to_numpy())\n",
        "      tmp=torch.from_numpy(tmp)  \n",
        "      tmp[tmp<=0.4]=0\n",
        "      #print([tmp.type(torch.FloatTensor),torch.ones(1).type(torch.long)])\n",
        "      G = nx.from_numpy_matrix(np.array(tmp))\n",
        "      centrality = nx.eigenvector_centrality(G)\n",
        "      x = []\n",
        "      for i in range(len(centrality)):\n",
        "        x.append([centrality[i]])\n",
        "      x = np.array(x).reshape(90,1)\n",
        "      data = from_networkx(G)\n",
        "      #x = torch.tensor([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1] ], dtype=torch.float)\n",
        "      data.x = torch.FloatTensor(x)\n",
        "      data.y = torch.tensor([1])\n",
        "      graphlist.append(data)\n",
        "      #graphlist.append([tmp.type(torch.FloatTensor),torch.ones(1).type(torch.long)])\n",
        "\n",
        "%cd /content/drive/MyDrive/ADNI2/Patients\n",
        "for i in range(1, 110):\n",
        "    if os.path.isfile('{0:0>3}'.format(i)+'.txt'):\n",
        "      tmp=pd.read_csv('{0:0>3}'.format(i)+'.txt', header=None)\n",
        "      tmp=(tmp.to_numpy())\n",
        "      tmp=torch.from_numpy(tmp) \n",
        "      tmp[tmp<=0.4]=0\n",
        "      #print([tmp.type(torch.FloatTensor),torch.ones(1).type(torch.long)])\n",
        "      G = nx.from_numpy_matrix(np.array(tmp))\n",
        "      centrality = nx.eigenvector_centrality(G)\n",
        "      x = []\n",
        "      for i in range(len(centrality)):\n",
        "        x.append([centrality[i]])\n",
        "      x = np.array(x).reshape(90,1)\n",
        "      data = from_networkx(G)\n",
        "      #x = torch.tensor([[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1],[1] ], dtype=torch.float)\n",
        "      data.x = torch.FloatTensor(x)\n",
        "      data.y = torch.tensor([0])\n",
        "      graphlist.append(data)\n",
        "      #graphlist.append([data, torch.zeros(1).type(torch.long)])\n",
        "\n",
        "      #graphlist.append([tmp.type(torch.FloatTensor),torch.zeros(1).type(torch.long)])"
      ],
      "metadata": {
        "id": "bzcY4tZ6JnAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ebe204-a492-47d2-c01e-1ddc34a86ab6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1n9P-S3rsi_g9yeb5KW1h7v-Bte7Cy43G/ADNI2/Controls\n",
            "/content/drive/.shortcut-targets-by-id/1n9P-S3rsi_g9yeb5KW1h7v-Bte7Cy43G/ADNI2/Patients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(graphlist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKi7rwVzCM_2",
        "outputId": "493c1026-f431-4b74-e212-62ee15b8a74d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in graphlist:\n",
        "  i.pos = torch.FloatTensor(position)"
      ],
      "metadata": {
        "id": "SgmINYahDPEY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "dataset = random.sample(graphlist, len(graphlist))\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=True)"
      ],
      "metadata": {
        "id": "HP_All4ZKhbb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.utils import normalized_cut\n",
        "\n",
        "\n",
        "def normalized_cut_2d(edge_index, pos):\n",
        "    row, col = edge_index\n",
        "    edge_attr = torch.norm(pos[row] - pos[col], p=2, dim=1)\n",
        "    return normalized_cut(edge_index, edge_attr, num_nodes=pos.size(0))"
      ],
      "metadata": {
        "id": "T2kPvAqdEiu3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops\n",
        "\n",
        "\n",
        "class SpatialGraphConv(MessagePassing):\n",
        "    def __init__(self, coors, in_channels, out_channels, hidden_size, dropout=0):\n",
        "        \"\"\"\n",
        "        coors - dimension of positional descriptors (e.g. 2 for 2D images)\n",
        "        in_channels - number of the input channels (node features)\n",
        "        out_channels - number of the output channels (node features)\n",
        "        hidden_size - number of the inner convolutions\n",
        "        dropout - dropout rate after the layer\n",
        "        \"\"\"\n",
        "        super(SpatialGraphConv, self).__init__(aggr='add')\n",
        "        self.dropout = dropout\n",
        "        self.lin_in = torch.nn.Linear(coors, hidden_size * in_channels)\n",
        "        self.lin_out = torch.nn.Linear(hidden_size * in_channels, out_channels)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "    def forward(self, x, pos, edge_index):\n",
        "        \"\"\"\n",
        "        x - feature matrix of the whole graph [num_nodes, label_dim]\n",
        "        pos - node position matrix [num_nodes, coors]\n",
        "        edge_index - graph connectivity [2, num_edges]\n",
        "        \"\"\"\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))  # num_edges = num_edges + num_nodes\n",
        "\n",
        "        return self.propagate(edge_index=edge_index, x=x, pos=pos, aggr='add')  # [N, out_channels, label_dim]\n",
        "\n",
        "    def message(self, pos_i, pos_j, x_j):\n",
        "        \"\"\"\n",
        "        pos_i [num_edges, coors]\n",
        "        pos_j [num_edges, coors]\n",
        "        x_j [num_edges, label_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        relative_pos = pos_j - pos_i  # [n_edges, hidden_size * in_channels]\n",
        "        spatial_scaling = F.relu(self.lin_in(relative_pos))  # [n_edges, hidden_size * in_channels]\n",
        "\n",
        "        n_edges = spatial_scaling.size(0)\n",
        "        # [n_edges, in_channels, ...] * [n_edges, in_channels, 1]\n",
        "        result = spatial_scaling.reshape(n_edges, self.in_channels, -1) * x_j.unsqueeze(-1)\n",
        "        return result.view(n_edges, -1)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        \"\"\"\n",
        "        aggr_out [num_nodes, label_dim, out_channels]\n",
        "        \"\"\"\n",
        "        aggr_out = self.lin_out(aggr_out)  # [num_nodes, label_dim, out_features]\n",
        "        aggr_out = F.relu(aggr_out)\n",
        "        aggr_out = F.dropout(aggr_out, p=self.dropout, training=self.training)\n",
        "\n",
        "        return aggr_out"
      ],
      "metadata": {
        "id": "Z0WMBOzLDsJB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import graclus, max_pool, global_mean_pool\n",
        "\n",
        "class SGCN(torch.nn.Module):\n",
        "    def __init__(self, dim_coor, out_dim, input_features,\n",
        "                 layers_num, model_dim, out_channels_1, dropout,\n",
        "                 use_cluster_pooling):\n",
        "        super(SGCN, self).__init__()\n",
        "        self.layers_num = layers_num\n",
        "        self.use_cluster_pooling = use_cluster_pooling\n",
        "\n",
        "        self.conv_layers = [SpatialGraphConv(coors=dim_coor,\n",
        "                                             in_channels=input_features,\n",
        "                                             out_channels=model_dim,\n",
        "                                             hidden_size=out_channels_1,\n",
        "                                             dropout=dropout)] + \\\n",
        "                           [SpatialGraphConv(coors=dim_coor,\n",
        "                                             in_channels=model_dim,\n",
        "                                             out_channels=model_dim,\n",
        "                                             hidden_size=out_channels_1,\n",
        "                                             dropout=dropout) for _ in range(layers_num - 1)]\n",
        "\n",
        "        self.conv_layers = torch.nn.ModuleList(self.conv_layers)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(model_dim, out_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        for i in range(self.layers_num):\n",
        "            data.x = self.conv_layers[i](data.x, data.pos, data.edge_index)\n",
        "\n",
        "            if self.use_cluster_pooling:\n",
        "                weight = normalized_cut_2d(data.edge_index, data.pos)\n",
        "                cluster = graclus(data.edge_index, weight, data.x.size(0))\n",
        "                data = max_pool(cluster, data, transform=T.Cartesian(cat=False))\n",
        "\n",
        "        data.x = global_mean_pool(data.x, data.batch)\n",
        "        x = self.fc1(data.x)\n",
        "\n",
        "        #return x\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "pQSvTFThEuO1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "sig=torch.nn.Sigmoid()\n",
        "model = SGCN(dim_coor=3,\n",
        "             out_dim=2,\n",
        "             input_features=1,\n",
        "             layers_num=3,\n",
        "             model_dim=16,\n",
        "             out_channels_1=64,\n",
        "             dropout=0.3,\n",
        "             use_cluster_pooling=False).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "#rotation_0 = T.RandomRotate(degrees=180, axis=0)\n",
        "#rotation_1 = T.RandomRotate(degrees=180, axis=1)\n",
        "#rotation_2 = T.RandomRotate(degrees=180, axis=2)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "\n",
        "    loss_all = 0\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        #output = output.view(-1)\n",
        "        #loss = criterion(output, data.y.float()) \n",
        "        loss = F.nll_loss(output, data.y)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        #out = model(data).max(dim=1)[1]\n",
        "        #pred = sig(out)>0.5\n",
        "        pred = model(data).max(dim=1)[1]\n",
        "        correct += pred.eq(data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "train_acc_array = []\n",
        "test_acc_array = []\n",
        "\n",
        "for epoch in range(1, 1000):\n",
        "    loss = train(epoch)\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "\n",
        "    train_acc_array.append(train_acc)\n",
        "    test_acc_array.append(test_acc)\n",
        "\n",
        "    print('Epoch: {:03d}, Loss: {:.5f}, Train Acc: {:.5f}, Test Acc: {:.5f}'.format(epoch, loss, train_acc, test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpFeOVMJFF-O",
        "outputId": "3faa2c8e-e53c-4338-d753-f1ad75e6859c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 175968.73730, Train Acc: 0.48667, Test Acc: 0.52941\n",
            "Epoch: 002, Loss: 10342.56567, Train Acc: 0.44667, Test Acc: 0.44118\n",
            "Epoch: 003, Loss: 3104.42991, Train Acc: 0.69333, Test Acc: 0.76471\n",
            "Epoch: 004, Loss: 1190.49138, Train Acc: 0.58667, Test Acc: 0.61765\n",
            "Epoch: 005, Loss: 1025.78937, Train Acc: 0.64667, Test Acc: 0.72059\n",
            "Epoch: 006, Loss: 640.60259, Train Acc: 0.66667, Test Acc: 0.67647\n",
            "Epoch: 007, Loss: 401.15643, Train Acc: 0.66667, Test Acc: 0.70588\n",
            "Epoch: 008, Loss: 335.88449, Train Acc: 0.68667, Test Acc: 0.66176\n",
            "Epoch: 009, Loss: 229.70531, Train Acc: 0.72000, Test Acc: 0.70588\n",
            "Epoch: 010, Loss: 208.70945, Train Acc: 0.76667, Test Acc: 0.76471\n",
            "Epoch: 011, Loss: 254.45156, Train Acc: 0.80667, Test Acc: 0.80882\n",
            "Epoch: 012, Loss: 163.48220, Train Acc: 0.72000, Test Acc: 0.72059\n",
            "Epoch: 013, Loss: 122.92592, Train Acc: 0.79333, Test Acc: 0.77941\n",
            "Epoch: 014, Loss: 119.04524, Train Acc: 0.80667, Test Acc: 0.77941\n",
            "Epoch: 015, Loss: 105.56549, Train Acc: 0.84000, Test Acc: 0.79412\n",
            "Epoch: 016, Loss: 100.83976, Train Acc: 0.74000, Test Acc: 0.73529\n",
            "Epoch: 017, Loss: 89.44298, Train Acc: 0.76667, Test Acc: 0.76471\n",
            "Epoch: 018, Loss: 71.79017, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 019, Loss: 57.93018, Train Acc: 0.80667, Test Acc: 0.83824\n",
            "Epoch: 020, Loss: 67.88557, Train Acc: 0.80000, Test Acc: 0.82353\n",
            "Epoch: 021, Loss: 50.15462, Train Acc: 0.83333, Test Acc: 0.82353\n",
            "Epoch: 022, Loss: 69.95248, Train Acc: 0.86000, Test Acc: 0.83824\n",
            "Epoch: 023, Loss: 38.19750, Train Acc: 0.86000, Test Acc: 0.83824\n",
            "Epoch: 024, Loss: 39.53815, Train Acc: 0.72667, Test Acc: 0.73529\n",
            "Epoch: 025, Loss: 47.76135, Train Acc: 0.82000, Test Acc: 0.82353\n",
            "Epoch: 026, Loss: 49.29228, Train Acc: 0.88000, Test Acc: 0.82353\n",
            "Epoch: 027, Loss: 35.54437, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 028, Loss: 35.81501, Train Acc: 0.88000, Test Acc: 0.85294\n",
            "Epoch: 029, Loss: 26.94830, Train Acc: 0.80667, Test Acc: 0.83824\n",
            "Epoch: 030, Loss: 45.24070, Train Acc: 0.86000, Test Acc: 0.86765\n",
            "Epoch: 031, Loss: 33.27863, Train Acc: 0.90000, Test Acc: 0.83824\n",
            "Epoch: 032, Loss: 33.82643, Train Acc: 0.89333, Test Acc: 0.83824\n",
            "Epoch: 033, Loss: 34.54723, Train Acc: 0.89333, Test Acc: 0.86765\n",
            "Epoch: 034, Loss: 25.61995, Train Acc: 0.88667, Test Acc: 0.86765\n",
            "Epoch: 035, Loss: 28.13404, Train Acc: 0.90000, Test Acc: 0.86765\n",
            "Epoch: 036, Loss: 21.97519, Train Acc: 0.88000, Test Acc: 0.86765\n",
            "Epoch: 037, Loss: 23.55646, Train Acc: 0.88000, Test Acc: 0.86765\n",
            "Epoch: 038, Loss: 33.62250, Train Acc: 0.85333, Test Acc: 0.88235\n",
            "Epoch: 039, Loss: 32.32586, Train Acc: 0.84667, Test Acc: 0.89706\n",
            "Epoch: 040, Loss: 12.38927, Train Acc: 0.88000, Test Acc: 0.92647\n",
            "Epoch: 041, Loss: 25.76515, Train Acc: 0.87333, Test Acc: 0.91176\n",
            "Epoch: 042, Loss: 19.65587, Train Acc: 0.88667, Test Acc: 0.89706\n",
            "Epoch: 043, Loss: 20.07970, Train Acc: 0.90000, Test Acc: 0.89706\n",
            "Epoch: 044, Loss: 20.15804, Train Acc: 0.89333, Test Acc: 0.91176\n",
            "Epoch: 045, Loss: 18.66543, Train Acc: 0.89333, Test Acc: 0.91176\n",
            "Epoch: 046, Loss: 18.58615, Train Acc: 0.87333, Test Acc: 0.91176\n",
            "Epoch: 047, Loss: 21.12161, Train Acc: 0.86667, Test Acc: 0.91176\n",
            "Epoch: 048, Loss: 14.23552, Train Acc: 0.88000, Test Acc: 0.92647\n",
            "Epoch: 049, Loss: 15.10927, Train Acc: 0.87333, Test Acc: 0.92647\n",
            "Epoch: 050, Loss: 23.39385, Train Acc: 0.87333, Test Acc: 0.91176\n",
            "Epoch: 051, Loss: 12.23387, Train Acc: 0.88000, Test Acc: 0.92647\n",
            "Epoch: 052, Loss: 13.97490, Train Acc: 0.90667, Test Acc: 0.92647\n",
            "Epoch: 053, Loss: 18.61652, Train Acc: 0.88000, Test Acc: 0.91176\n",
            "Epoch: 054, Loss: 14.92127, Train Acc: 0.86667, Test Acc: 0.89706\n",
            "Epoch: 055, Loss: 11.83708, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 056, Loss: 13.54486, Train Acc: 0.91333, Test Acc: 0.92647\n",
            "Epoch: 057, Loss: 16.65736, Train Acc: 0.91333, Test Acc: 0.94118\n",
            "Epoch: 058, Loss: 13.62267, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 059, Loss: 13.56642, Train Acc: 0.88667, Test Acc: 0.89706\n",
            "Epoch: 060, Loss: 10.81372, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 061, Loss: 10.02411, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 062, Loss: 10.06225, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 063, Loss: 10.35243, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 064, Loss: 7.67588, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 065, Loss: 9.33568, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 066, Loss: 7.98100, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 067, Loss: 5.65211, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 068, Loss: 13.26095, Train Acc: 0.85333, Test Acc: 0.86765\n",
            "Epoch: 069, Loss: 11.41749, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 070, Loss: 6.87582, Train Acc: 0.80667, Test Acc: 0.85294\n",
            "Epoch: 071, Loss: 7.76291, Train Acc: 0.80667, Test Acc: 0.85294\n",
            "Epoch: 072, Loss: 12.25709, Train Acc: 0.82000, Test Acc: 0.86765\n",
            "Epoch: 073, Loss: 8.13086, Train Acc: 0.83333, Test Acc: 0.88235\n",
            "Epoch: 074, Loss: 6.36251, Train Acc: 0.82000, Test Acc: 0.88235\n",
            "Epoch: 075, Loss: 11.45182, Train Acc: 0.81333, Test Acc: 0.89706\n",
            "Epoch: 076, Loss: 8.38753, Train Acc: 0.81333, Test Acc: 0.89706\n",
            "Epoch: 077, Loss: 6.90054, Train Acc: 0.80667, Test Acc: 0.89706\n",
            "Epoch: 078, Loss: 7.46764, Train Acc: 0.80667, Test Acc: 0.91176\n",
            "Epoch: 079, Loss: 9.56858, Train Acc: 0.79333, Test Acc: 0.91176\n",
            "Epoch: 080, Loss: 5.36307, Train Acc: 0.78000, Test Acc: 0.91176\n",
            "Epoch: 081, Loss: 6.57829, Train Acc: 0.78667, Test Acc: 0.91176\n",
            "Epoch: 082, Loss: 4.87759, Train Acc: 0.78667, Test Acc: 0.91176\n",
            "Epoch: 083, Loss: 8.65971, Train Acc: 0.78000, Test Acc: 0.89706\n",
            "Epoch: 084, Loss: 8.37962, Train Acc: 0.78000, Test Acc: 0.86765\n",
            "Epoch: 085, Loss: 9.22760, Train Acc: 0.79333, Test Acc: 0.85294\n",
            "Epoch: 086, Loss: 4.72781, Train Acc: 0.79333, Test Acc: 0.85294\n",
            "Epoch: 087, Loss: 7.26832, Train Acc: 0.80000, Test Acc: 0.85294\n",
            "Epoch: 088, Loss: 7.85989, Train Acc: 0.83333, Test Acc: 0.89706\n",
            "Epoch: 089, Loss: 6.39382, Train Acc: 0.84000, Test Acc: 0.89706\n",
            "Epoch: 090, Loss: 9.84759, Train Acc: 0.82667, Test Acc: 0.88235\n",
            "Epoch: 091, Loss: 8.32190, Train Acc: 0.80667, Test Acc: 0.86765\n",
            "Epoch: 092, Loss: 6.78753, Train Acc: 0.79333, Test Acc: 0.83824\n",
            "Epoch: 093, Loss: 6.57363, Train Acc: 0.79333, Test Acc: 0.83824\n",
            "Epoch: 094, Loss: 6.99431, Train Acc: 0.80000, Test Acc: 0.86765\n",
            "Epoch: 095, Loss: 6.69485, Train Acc: 0.80667, Test Acc: 0.86765\n",
            "Epoch: 096, Loss: 5.16521, Train Acc: 0.80667, Test Acc: 0.83824\n",
            "Epoch: 097, Loss: 7.62882, Train Acc: 0.81333, Test Acc: 0.83824\n",
            "Epoch: 098, Loss: 7.19670, Train Acc: 0.80000, Test Acc: 0.83824\n",
            "Epoch: 099, Loss: 5.15433, Train Acc: 0.80000, Test Acc: 0.83824\n",
            "Epoch: 100, Loss: 7.06536, Train Acc: 0.79333, Test Acc: 0.83824\n",
            "Epoch: 101, Loss: 4.76296, Train Acc: 0.78667, Test Acc: 0.83824\n",
            "Epoch: 102, Loss: 5.81569, Train Acc: 0.80000, Test Acc: 0.89706\n",
            "Epoch: 103, Loss: 3.62334, Train Acc: 0.81333, Test Acc: 0.89706\n",
            "Epoch: 104, Loss: 3.31972, Train Acc: 0.82667, Test Acc: 0.89706\n",
            "Epoch: 105, Loss: 6.37657, Train Acc: 0.81333, Test Acc: 0.86765\n",
            "Epoch: 106, Loss: 4.02238, Train Acc: 0.78667, Test Acc: 0.85294\n",
            "Epoch: 107, Loss: 4.89375, Train Acc: 0.78667, Test Acc: 0.85294\n",
            "Epoch: 108, Loss: 3.89579, Train Acc: 0.78667, Test Acc: 0.83824\n",
            "Epoch: 109, Loss: 4.70912, Train Acc: 0.79333, Test Acc: 0.83824\n",
            "Epoch: 110, Loss: 5.06411, Train Acc: 0.80667, Test Acc: 0.82353\n",
            "Epoch: 111, Loss: 6.33904, Train Acc: 0.80667, Test Acc: 0.85294\n",
            "Epoch: 112, Loss: 4.76738, Train Acc: 0.82000, Test Acc: 0.85294\n",
            "Epoch: 113, Loss: 5.11317, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 114, Loss: 3.22266, Train Acc: 0.86667, Test Acc: 0.85294\n",
            "Epoch: 115, Loss: 5.30960, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 116, Loss: 3.87444, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 117, Loss: 3.19618, Train Acc: 0.82000, Test Acc: 0.83824\n",
            "Epoch: 118, Loss: 3.66127, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 119, Loss: 3.67813, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 120, Loss: 3.54312, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 121, Loss: 4.54402, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 122, Loss: 4.61300, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 123, Loss: 3.76810, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 124, Loss: 2.99793, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 125, Loss: 3.84928, Train Acc: 0.81333, Test Acc: 0.83824\n",
            "Epoch: 126, Loss: 3.53079, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 127, Loss: 4.08051, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 128, Loss: 3.79999, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 129, Loss: 4.78798, Train Acc: 0.82000, Test Acc: 0.83824\n",
            "Epoch: 130, Loss: 2.98681, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 131, Loss: 3.92682, Train Acc: 0.84667, Test Acc: 0.83824\n",
            "Epoch: 132, Loss: 2.78943, Train Acc: 0.82667, Test Acc: 0.82353\n",
            "Epoch: 133, Loss: 4.03112, Train Acc: 0.80667, Test Acc: 0.82353\n",
            "Epoch: 134, Loss: 3.37959, Train Acc: 0.80667, Test Acc: 0.82353\n",
            "Epoch: 135, Loss: 2.05013, Train Acc: 0.80000, Test Acc: 0.82353\n",
            "Epoch: 136, Loss: 3.53270, Train Acc: 0.82000, Test Acc: 0.82353\n",
            "Epoch: 137, Loss: 2.33344, Train Acc: 0.82000, Test Acc: 0.82353\n",
            "Epoch: 138, Loss: 3.90574, Train Acc: 0.82667, Test Acc: 0.82353\n",
            "Epoch: 139, Loss: 1.90810, Train Acc: 0.83333, Test Acc: 0.82353\n",
            "Epoch: 140, Loss: 2.37353, Train Acc: 0.84667, Test Acc: 0.83824\n",
            "Epoch: 141, Loss: 4.00163, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 142, Loss: 2.99115, Train Acc: 0.82667, Test Acc: 0.82353\n",
            "Epoch: 143, Loss: 2.21705, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 144, Loss: 2.66491, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 145, Loss: 2.54355, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 146, Loss: 2.79587, Train Acc: 0.86667, Test Acc: 0.83824\n",
            "Epoch: 147, Loss: 1.71146, Train Acc: 0.84667, Test Acc: 0.83824\n",
            "Epoch: 148, Loss: 2.65993, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 149, Loss: 2.39031, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 150, Loss: 3.43294, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 151, Loss: 3.59424, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 152, Loss: 1.33580, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 153, Loss: 2.43410, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 154, Loss: 2.11130, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 155, Loss: 2.18391, Train Acc: 0.84000, Test Acc: 0.82353\n",
            "Epoch: 156, Loss: 1.55710, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 157, Loss: 1.71118, Train Acc: 0.81333, Test Acc: 0.83824\n",
            "Epoch: 158, Loss: 2.17168, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 159, Loss: 2.90682, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 160, Loss: 3.64581, Train Acc: 0.82000, Test Acc: 0.85294\n",
            "Epoch: 161, Loss: 2.28196, Train Acc: 0.80667, Test Acc: 0.85294\n",
            "Epoch: 162, Loss: 1.47034, Train Acc: 0.86000, Test Acc: 0.86765\n",
            "Epoch: 163, Loss: 1.54465, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 164, Loss: 2.15642, Train Acc: 0.80000, Test Acc: 0.82353\n",
            "Epoch: 165, Loss: 2.14652, Train Acc: 0.80667, Test Acc: 0.80882\n",
            "Epoch: 166, Loss: 2.07790, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 167, Loss: 2.74161, Train Acc: 0.84667, Test Acc: 0.82353\n",
            "Epoch: 168, Loss: 3.53437, Train Acc: 0.84667, Test Acc: 0.82353\n",
            "Epoch: 169, Loss: 1.63457, Train Acc: 0.81333, Test Acc: 0.83824\n",
            "Epoch: 170, Loss: 1.74065, Train Acc: 0.80667, Test Acc: 0.83824\n",
            "Epoch: 171, Loss: 2.21464, Train Acc: 0.81333, Test Acc: 0.85294\n",
            "Epoch: 172, Loss: 2.08129, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 173, Loss: 1.43265, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 174, Loss: 1.35095, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 175, Loss: 2.00879, Train Acc: 0.82000, Test Acc: 0.86765\n",
            "Epoch: 176, Loss: 1.36873, Train Acc: 0.82000, Test Acc: 0.88235\n",
            "Epoch: 177, Loss: 1.37380, Train Acc: 0.86667, Test Acc: 0.85294\n",
            "Epoch: 178, Loss: 1.79665, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 179, Loss: 1.64758, Train Acc: 0.84000, Test Acc: 0.86765\n",
            "Epoch: 180, Loss: 1.22690, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 181, Loss: 1.22137, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 182, Loss: 1.78085, Train Acc: 0.86000, Test Acc: 0.83824\n",
            "Epoch: 183, Loss: 0.91824, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 184, Loss: 1.93551, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 185, Loss: 2.18731, Train Acc: 0.84000, Test Acc: 0.86765\n",
            "Epoch: 186, Loss: 1.39372, Train Acc: 0.83333, Test Acc: 0.88235\n",
            "Epoch: 187, Loss: 1.72802, Train Acc: 0.81333, Test Acc: 0.85294\n",
            "Epoch: 188, Loss: 1.63560, Train Acc: 0.80667, Test Acc: 0.85294\n",
            "Epoch: 189, Loss: 1.54213, Train Acc: 0.83333, Test Acc: 0.88235\n",
            "Epoch: 190, Loss: 1.53603, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 191, Loss: 1.66195, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 192, Loss: 1.18287, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 193, Loss: 1.53565, Train Acc: 0.88667, Test Acc: 0.82353\n",
            "Epoch: 194, Loss: 1.32345, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 195, Loss: 1.67235, Train Acc: 0.86667, Test Acc: 0.83824\n",
            "Epoch: 196, Loss: 1.13071, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 197, Loss: 1.18042, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 198, Loss: 1.62384, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 199, Loss: 1.41029, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 200, Loss: 1.38137, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 201, Loss: 2.00992, Train Acc: 0.86000, Test Acc: 0.85294\n",
            "Epoch: 202, Loss: 1.35622, Train Acc: 0.86667, Test Acc: 0.85294\n",
            "Epoch: 203, Loss: 2.48569, Train Acc: 0.86000, Test Acc: 0.83824\n",
            "Epoch: 204, Loss: 1.63581, Train Acc: 0.86667, Test Acc: 0.83824\n",
            "Epoch: 205, Loss: 1.45860, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 206, Loss: 1.53952, Train Acc: 0.86667, Test Acc: 0.85294\n",
            "Epoch: 207, Loss: 1.64402, Train Acc: 0.85333, Test Acc: 0.86765\n",
            "Epoch: 208, Loss: 0.80111, Train Acc: 0.87333, Test Acc: 0.86765\n",
            "Epoch: 209, Loss: 0.82721, Train Acc: 0.86000, Test Acc: 0.88235\n",
            "Epoch: 210, Loss: 1.37533, Train Acc: 0.86667, Test Acc: 0.86765\n",
            "Epoch: 211, Loss: 1.05192, Train Acc: 0.88000, Test Acc: 0.86765\n",
            "Epoch: 212, Loss: 1.12013, Train Acc: 0.88000, Test Acc: 0.86765\n",
            "Epoch: 213, Loss: 1.11975, Train Acc: 0.86667, Test Acc: 0.86765\n",
            "Epoch: 214, Loss: 1.16824, Train Acc: 0.84000, Test Acc: 0.86765\n",
            "Epoch: 215, Loss: 1.75104, Train Acc: 0.82667, Test Acc: 0.86765\n",
            "Epoch: 216, Loss: 1.04461, Train Acc: 0.82000, Test Acc: 0.88235\n",
            "Epoch: 217, Loss: 1.72883, Train Acc: 0.84000, Test Acc: 0.86765\n",
            "Epoch: 218, Loss: 1.24609, Train Acc: 0.85333, Test Acc: 0.86765\n",
            "Epoch: 219, Loss: 1.28278, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 220, Loss: 0.85491, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 221, Loss: 1.03313, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 222, Loss: 0.93099, Train Acc: 0.83333, Test Acc: 0.86765\n",
            "Epoch: 223, Loss: 0.94319, Train Acc: 0.85333, Test Acc: 0.86765\n",
            "Epoch: 224, Loss: 0.65068, Train Acc: 0.86000, Test Acc: 0.86765\n",
            "Epoch: 225, Loss: 1.10523, Train Acc: 0.84000, Test Acc: 0.86765\n",
            "Epoch: 226, Loss: 0.96941, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 227, Loss: 1.01250, Train Acc: 0.80000, Test Acc: 0.89706\n",
            "Epoch: 228, Loss: 0.64654, Train Acc: 0.81333, Test Acc: 0.89706\n",
            "Epoch: 229, Loss: 1.03069, Train Acc: 0.83333, Test Acc: 0.88235\n",
            "Epoch: 230, Loss: 0.60611, Train Acc: 0.82667, Test Acc: 0.86765\n",
            "Epoch: 231, Loss: 0.67832, Train Acc: 0.82000, Test Acc: 0.88235\n",
            "Epoch: 232, Loss: 1.01909, Train Acc: 0.82667, Test Acc: 0.86765\n",
            "Epoch: 233, Loss: 0.97345, Train Acc: 0.82667, Test Acc: 0.86765\n",
            "Epoch: 234, Loss: 0.90674, Train Acc: 0.83333, Test Acc: 0.86765\n",
            "Epoch: 235, Loss: 0.82465, Train Acc: 0.82667, Test Acc: 0.86765\n",
            "Epoch: 236, Loss: 1.13761, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 237, Loss: 0.76692, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 238, Loss: 0.98687, Train Acc: 0.82667, Test Acc: 0.86765\n",
            "Epoch: 239, Loss: 1.01198, Train Acc: 0.81333, Test Acc: 0.88235\n",
            "Epoch: 240, Loss: 0.77157, Train Acc: 0.79333, Test Acc: 0.85294\n",
            "Epoch: 241, Loss: 0.89998, Train Acc: 0.79333, Test Acc: 0.85294\n",
            "Epoch: 242, Loss: 0.90825, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 243, Loss: 0.87032, Train Acc: 0.83333, Test Acc: 0.82353\n",
            "Epoch: 244, Loss: 0.74001, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 245, Loss: 0.65316, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 246, Loss: 0.75575, Train Acc: 0.80000, Test Acc: 0.85294\n",
            "Epoch: 247, Loss: 0.91036, Train Acc: 0.82000, Test Acc: 0.85294\n",
            "Epoch: 248, Loss: 0.80220, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 249, Loss: 0.59106, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 250, Loss: 0.86548, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 251, Loss: 0.65236, Train Acc: 0.81333, Test Acc: 0.85294\n",
            "Epoch: 252, Loss: 0.76053, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 253, Loss: 0.82556, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 254, Loss: 1.03896, Train Acc: 0.82000, Test Acc: 0.83824\n",
            "Epoch: 255, Loss: 0.80913, Train Acc: 0.81333, Test Acc: 0.83824\n",
            "Epoch: 256, Loss: 0.61828, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 257, Loss: 1.01955, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 258, Loss: 0.66366, Train Acc: 0.84667, Test Acc: 0.82353\n",
            "Epoch: 259, Loss: 0.86395, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 260, Loss: 0.63971, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 261, Loss: 0.98215, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 262, Loss: 0.82394, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 263, Loss: 0.90163, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 264, Loss: 0.87642, Train Acc: 0.84000, Test Acc: 0.79412\n",
            "Epoch: 265, Loss: 1.13512, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 266, Loss: 0.59504, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 267, Loss: 0.54446, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 268, Loss: 0.82892, Train Acc: 0.82667, Test Acc: 0.77941\n",
            "Epoch: 269, Loss: 0.51192, Train Acc: 0.80667, Test Acc: 0.77941\n",
            "Epoch: 270, Loss: 0.68058, Train Acc: 0.84000, Test Acc: 0.79412\n",
            "Epoch: 271, Loss: 0.56043, Train Acc: 0.88667, Test Acc: 0.77941\n",
            "Epoch: 272, Loss: 0.70722, Train Acc: 0.85333, Test Acc: 0.79412\n",
            "Epoch: 273, Loss: 0.65717, Train Acc: 0.84667, Test Acc: 0.80882\n",
            "Epoch: 274, Loss: 0.48097, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 275, Loss: 0.52505, Train Acc: 0.81333, Test Acc: 0.80882\n",
            "Epoch: 276, Loss: 0.67191, Train Acc: 0.82667, Test Acc: 0.80882\n",
            "Epoch: 277, Loss: 0.72133, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 278, Loss: 0.55843, Train Acc: 0.84667, Test Acc: 0.79412\n",
            "Epoch: 279, Loss: 0.67760, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 280, Loss: 0.68312, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 281, Loss: 0.61276, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 282, Loss: 0.56160, Train Acc: 0.86667, Test Acc: 0.83824\n",
            "Epoch: 283, Loss: 0.84090, Train Acc: 0.90000, Test Acc: 0.82353\n",
            "Epoch: 284, Loss: 0.63605, Train Acc: 0.90000, Test Acc: 0.82353\n",
            "Epoch: 285, Loss: 0.75119, Train Acc: 0.88000, Test Acc: 0.80882\n",
            "Epoch: 286, Loss: 0.71751, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 287, Loss: 0.51972, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 288, Loss: 0.45443, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 289, Loss: 0.75008, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 290, Loss: 0.60441, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 291, Loss: 0.61222, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 292, Loss: 0.60496, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 293, Loss: 0.65322, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 294, Loss: 0.67302, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 295, Loss: 0.66157, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 296, Loss: 0.87838, Train Acc: 0.88000, Test Acc: 0.80882\n",
            "Epoch: 297, Loss: 0.51678, Train Acc: 0.87333, Test Acc: 0.79412\n",
            "Epoch: 298, Loss: 0.46162, Train Acc: 0.88667, Test Acc: 0.79412\n",
            "Epoch: 299, Loss: 0.67768, Train Acc: 0.89333, Test Acc: 0.80882\n",
            "Epoch: 300, Loss: 0.73469, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 301, Loss: 0.55493, Train Acc: 0.88667, Test Acc: 0.82353\n",
            "Epoch: 302, Loss: 0.46901, Train Acc: 0.84000, Test Acc: 0.79412\n",
            "Epoch: 303, Loss: 0.89240, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 304, Loss: 0.42961, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 305, Loss: 0.51128, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 306, Loss: 0.56143, Train Acc: 0.88000, Test Acc: 0.80882\n",
            "Epoch: 307, Loss: 0.59196, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 308, Loss: 0.57545, Train Acc: 0.84000, Test Acc: 0.82353\n",
            "Epoch: 309, Loss: 0.69114, Train Acc: 0.84667, Test Acc: 0.80882\n",
            "Epoch: 310, Loss: 0.54568, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 311, Loss: 0.58733, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 312, Loss: 0.66249, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 313, Loss: 0.69090, Train Acc: 0.83333, Test Acc: 0.79412\n",
            "Epoch: 314, Loss: 0.61001, Train Acc: 0.84667, Test Acc: 0.80882\n",
            "Epoch: 315, Loss: 0.43901, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 316, Loss: 0.54126, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 317, Loss: 0.52989, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 318, Loss: 0.81531, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 319, Loss: 0.54632, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 320, Loss: 0.62253, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 321, Loss: 0.77117, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 322, Loss: 0.52635, Train Acc: 0.88000, Test Acc: 0.82353\n",
            "Epoch: 323, Loss: 1.02079, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 324, Loss: 0.56746, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 325, Loss: 0.60359, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 326, Loss: 0.67093, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 327, Loss: 0.54212, Train Acc: 0.82667, Test Acc: 0.79412\n",
            "Epoch: 328, Loss: 0.41928, Train Acc: 0.88667, Test Acc: 0.80882\n",
            "Epoch: 329, Loss: 0.83844, Train Acc: 0.88000, Test Acc: 0.75000\n",
            "Epoch: 330, Loss: 0.64648, Train Acc: 0.88000, Test Acc: 0.80882\n",
            "Epoch: 331, Loss: 0.54384, Train Acc: 0.88667, Test Acc: 0.80882\n",
            "Epoch: 332, Loss: 0.64505, Train Acc: 0.79333, Test Acc: 0.77941\n",
            "Epoch: 333, Loss: 0.48647, Train Acc: 0.78667, Test Acc: 0.77941\n",
            "Epoch: 334, Loss: 0.57734, Train Acc: 0.85333, Test Acc: 0.80882\n",
            "Epoch: 335, Loss: 0.53077, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 336, Loss: 0.63483, Train Acc: 0.88667, Test Acc: 0.80882\n",
            "Epoch: 337, Loss: 0.42423, Train Acc: 0.88000, Test Acc: 0.80882\n",
            "Epoch: 338, Loss: 0.50080, Train Acc: 0.82667, Test Acc: 0.79412\n",
            "Epoch: 339, Loss: 0.63313, Train Acc: 0.84667, Test Acc: 0.79412\n",
            "Epoch: 340, Loss: 0.55673, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 341, Loss: 0.63472, Train Acc: 0.88667, Test Acc: 0.80882\n",
            "Epoch: 342, Loss: 0.58157, Train Acc: 0.88000, Test Acc: 0.77941\n",
            "Epoch: 343, Loss: 0.53350, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 344, Loss: 0.47326, Train Acc: 0.82000, Test Acc: 0.80882\n",
            "Epoch: 345, Loss: 0.46234, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 346, Loss: 0.69356, Train Acc: 0.87333, Test Acc: 0.80882\n",
            "Epoch: 347, Loss: 0.56367, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 348, Loss: 0.57829, Train Acc: 0.88667, Test Acc: 0.77941\n",
            "Epoch: 349, Loss: 0.52619, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 350, Loss: 0.53296, Train Acc: 0.86667, Test Acc: 0.80882\n",
            "Epoch: 351, Loss: 0.55063, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 352, Loss: 0.61022, Train Acc: 0.89333, Test Acc: 0.80882\n",
            "Epoch: 353, Loss: 0.51690, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 354, Loss: 0.43860, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 355, Loss: 0.46561, Train Acc: 0.84667, Test Acc: 0.80882\n",
            "Epoch: 356, Loss: 0.55105, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 357, Loss: 0.53595, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 358, Loss: 0.45714, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 359, Loss: 0.41960, Train Acc: 0.81333, Test Acc: 0.82353\n",
            "Epoch: 360, Loss: 0.65166, Train Acc: 0.78000, Test Acc: 0.79412\n",
            "Epoch: 361, Loss: 0.61296, Train Acc: 0.84000, Test Acc: 0.82353\n",
            "Epoch: 362, Loss: 0.53099, Train Acc: 0.90000, Test Acc: 0.82353\n",
            "Epoch: 363, Loss: 0.62086, Train Acc: 0.90667, Test Acc: 0.82353\n",
            "Epoch: 364, Loss: 0.35875, Train Acc: 0.89333, Test Acc: 0.83824\n",
            "Epoch: 365, Loss: 0.56740, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 366, Loss: 0.52898, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 367, Loss: 0.35598, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 368, Loss: 0.52968, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 369, Loss: 0.65423, Train Acc: 0.85333, Test Acc: 0.79412\n",
            "Epoch: 370, Loss: 0.49970, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 371, Loss: 0.53500, Train Acc: 0.90667, Test Acc: 0.80882\n",
            "Epoch: 372, Loss: 0.52907, Train Acc: 0.92000, Test Acc: 0.80882\n",
            "Epoch: 373, Loss: 0.44677, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 374, Loss: 0.86389, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 375, Loss: 0.47907, Train Acc: 0.90667, Test Acc: 0.76471\n",
            "Epoch: 376, Loss: 0.45589, Train Acc: 0.92000, Test Acc: 0.82353\n",
            "Epoch: 377, Loss: 0.55027, Train Acc: 0.89333, Test Acc: 0.85294\n",
            "Epoch: 378, Loss: 0.67587, Train Acc: 0.83333, Test Acc: 0.82353\n",
            "Epoch: 379, Loss: 0.45440, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 380, Loss: 0.48101, Train Acc: 0.88000, Test Acc: 0.85294\n",
            "Epoch: 381, Loss: 0.54217, Train Acc: 0.87333, Test Acc: 0.85294\n",
            "Epoch: 382, Loss: 0.47827, Train Acc: 0.88000, Test Acc: 0.85294\n",
            "Epoch: 383, Loss: 0.41978, Train Acc: 0.90667, Test Acc: 0.82353\n",
            "Epoch: 384, Loss: 0.48869, Train Acc: 0.90000, Test Acc: 0.79412\n",
            "Epoch: 385, Loss: 0.45379, Train Acc: 0.90000, Test Acc: 0.77941\n",
            "Epoch: 386, Loss: 0.65565, Train Acc: 0.89333, Test Acc: 0.79412\n",
            "Epoch: 387, Loss: 0.58223, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 388, Loss: 0.45053, Train Acc: 0.82000, Test Acc: 0.80882\n",
            "Epoch: 389, Loss: 0.43966, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 390, Loss: 0.49180, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 391, Loss: 0.46590, Train Acc: 0.90667, Test Acc: 0.77941\n",
            "Epoch: 392, Loss: 0.43739, Train Acc: 0.90000, Test Acc: 0.80882\n",
            "Epoch: 393, Loss: 0.54485, Train Acc: 0.92667, Test Acc: 0.82353\n",
            "Epoch: 394, Loss: 0.52994, Train Acc: 0.90000, Test Acc: 0.83824\n",
            "Epoch: 395, Loss: 0.51797, Train Acc: 0.92667, Test Acc: 0.83824\n",
            "Epoch: 396, Loss: 0.49555, Train Acc: 0.90000, Test Acc: 0.83824\n",
            "Epoch: 397, Loss: 0.59613, Train Acc: 0.84667, Test Acc: 0.83824\n",
            "Epoch: 398, Loss: 0.58906, Train Acc: 0.78667, Test Acc: 0.77941\n",
            "Epoch: 399, Loss: 0.54458, Train Acc: 0.91333, Test Acc: 0.82353\n",
            "Epoch: 400, Loss: 0.39749, Train Acc: 0.90667, Test Acc: 0.85294\n",
            "Epoch: 401, Loss: 0.51408, Train Acc: 0.76667, Test Acc: 0.76471\n",
            "Epoch: 402, Loss: 0.49245, Train Acc: 0.90000, Test Acc: 0.85294\n",
            "Epoch: 403, Loss: 0.46757, Train Acc: 0.91333, Test Acc: 0.83824\n",
            "Epoch: 404, Loss: 0.65600, Train Acc: 0.82667, Test Acc: 0.79412\n",
            "Epoch: 405, Loss: 0.47380, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 406, Loss: 0.47550, Train Acc: 0.92000, Test Acc: 0.80882\n",
            "Epoch: 407, Loss: 0.53262, Train Acc: 0.88000, Test Acc: 0.77941\n",
            "Epoch: 408, Loss: 0.47943, Train Acc: 0.86000, Test Acc: 0.82353\n",
            "Epoch: 409, Loss: 0.51300, Train Acc: 0.81333, Test Acc: 0.80882\n",
            "Epoch: 410, Loss: 0.53668, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 411, Loss: 0.46458, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 412, Loss: 0.48192, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 413, Loss: 0.61931, Train Acc: 0.82667, Test Acc: 0.82353\n",
            "Epoch: 414, Loss: 0.56273, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 415, Loss: 0.44024, Train Acc: 0.88000, Test Acc: 0.77941\n",
            "Epoch: 416, Loss: 0.58859, Train Acc: 0.92000, Test Acc: 0.79412\n",
            "Epoch: 417, Loss: 0.45969, Train Acc: 0.88000, Test Acc: 0.85294\n",
            "Epoch: 418, Loss: 0.54776, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 419, Loss: 0.60843, Train Acc: 0.90000, Test Acc: 0.76471\n",
            "Epoch: 420, Loss: 0.43985, Train Acc: 0.89333, Test Acc: 0.80882\n",
            "Epoch: 421, Loss: 0.41348, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 422, Loss: 0.48470, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 423, Loss: 0.40333, Train Acc: 0.91333, Test Acc: 0.80882\n",
            "Epoch: 424, Loss: 0.51254, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 425, Loss: 0.47880, Train Acc: 0.82000, Test Acc: 0.83824\n",
            "Epoch: 426, Loss: 0.44784, Train Acc: 0.89333, Test Acc: 0.83824\n",
            "Epoch: 427, Loss: 0.52871, Train Acc: 0.90000, Test Acc: 0.85294\n",
            "Epoch: 428, Loss: 0.51994, Train Acc: 0.87333, Test Acc: 0.85294\n",
            "Epoch: 429, Loss: 0.45645, Train Acc: 0.88000, Test Acc: 0.85294\n",
            "Epoch: 430, Loss: 0.43856, Train Acc: 0.90000, Test Acc: 0.82353\n",
            "Epoch: 431, Loss: 0.50153, Train Acc: 0.82667, Test Acc: 0.80882\n",
            "Epoch: 432, Loss: 0.38755, Train Acc: 0.92000, Test Acc: 0.80882\n",
            "Epoch: 433, Loss: 0.50597, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 434, Loss: 0.48567, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 435, Loss: 0.52769, Train Acc: 0.82000, Test Acc: 0.80882\n",
            "Epoch: 436, Loss: 0.53629, Train Acc: 0.89333, Test Acc: 0.83824\n",
            "Epoch: 437, Loss: 0.38596, Train Acc: 0.90000, Test Acc: 0.83824\n",
            "Epoch: 438, Loss: 0.38762, Train Acc: 0.86000, Test Acc: 0.85294\n",
            "Epoch: 439, Loss: 0.47302, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 440, Loss: 0.47595, Train Acc: 0.93333, Test Acc: 0.83824\n",
            "Epoch: 441, Loss: 0.44462, Train Acc: 0.80667, Test Acc: 0.80882\n",
            "Epoch: 442, Loss: 0.43185, Train Acc: 0.91333, Test Acc: 0.83824\n",
            "Epoch: 443, Loss: 0.50917, Train Acc: 0.81333, Test Acc: 0.80882\n",
            "Epoch: 444, Loss: 0.46902, Train Acc: 0.79333, Test Acc: 0.80882\n",
            "Epoch: 445, Loss: 0.54391, Train Acc: 0.82000, Test Acc: 0.76471\n",
            "Epoch: 446, Loss: 0.50426, Train Acc: 0.81333, Test Acc: 0.82353\n",
            "Epoch: 447, Loss: 0.37562, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 448, Loss: 0.52824, Train Acc: 0.90667, Test Acc: 0.85294\n",
            "Epoch: 449, Loss: 0.39604, Train Acc: 0.92000, Test Acc: 0.86765\n",
            "Epoch: 450, Loss: 0.56651, Train Acc: 0.77333, Test Acc: 0.82353\n",
            "Epoch: 451, Loss: 0.52649, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 452, Loss: 0.47650, Train Acc: 0.91333, Test Acc: 0.83824\n",
            "Epoch: 453, Loss: 0.48255, Train Acc: 0.86000, Test Acc: 0.85294\n",
            "Epoch: 454, Loss: 0.41965, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 455, Loss: 0.36913, Train Acc: 0.84667, Test Acc: 0.83824\n",
            "Epoch: 456, Loss: 0.48757, Train Acc: 0.92000, Test Acc: 0.83824\n",
            "Epoch: 457, Loss: 0.52330, Train Acc: 0.88000, Test Acc: 0.80882\n",
            "Epoch: 458, Loss: 0.48028, Train Acc: 0.92000, Test Acc: 0.83824\n",
            "Epoch: 459, Loss: 0.53640, Train Acc: 0.92667, Test Acc: 0.83824\n",
            "Epoch: 460, Loss: 0.42248, Train Acc: 0.89333, Test Acc: 0.83824\n",
            "Epoch: 461, Loss: 0.42146, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 462, Loss: 0.41581, Train Acc: 0.84667, Test Acc: 0.82353\n",
            "Epoch: 463, Loss: 0.38206, Train Acc: 0.90000, Test Acc: 0.83824\n",
            "Epoch: 464, Loss: 0.37496, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 465, Loss: 0.49061, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 466, Loss: 0.56877, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 467, Loss: 0.45331, Train Acc: 0.88000, Test Acc: 0.82353\n",
            "Epoch: 468, Loss: 0.46850, Train Acc: 0.90667, Test Acc: 0.80882\n",
            "Epoch: 469, Loss: 0.40743, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 470, Loss: 0.43677, Train Acc: 0.90000, Test Acc: 0.86765\n",
            "Epoch: 471, Loss: 0.46374, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 472, Loss: 0.40140, Train Acc: 0.88667, Test Acc: 0.83824\n",
            "Epoch: 473, Loss: 0.44691, Train Acc: 0.80667, Test Acc: 0.75000\n",
            "Epoch: 474, Loss: 0.41834, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 475, Loss: 0.47060, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 476, Loss: 0.50453, Train Acc: 0.84667, Test Acc: 0.77941\n",
            "Epoch: 477, Loss: 0.39048, Train Acc: 0.85333, Test Acc: 0.79412\n",
            "Epoch: 478, Loss: 0.43129, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 479, Loss: 0.43488, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 480, Loss: 0.38862, Train Acc: 0.89333, Test Acc: 0.86765\n",
            "Epoch: 481, Loss: 0.46371, Train Acc: 0.88667, Test Acc: 0.86765\n",
            "Epoch: 482, Loss: 0.58598, Train Acc: 0.92000, Test Acc: 0.88235\n",
            "Epoch: 483, Loss: 0.41202, Train Acc: 0.79333, Test Acc: 0.83824\n",
            "Epoch: 484, Loss: 0.45492, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 485, Loss: 0.48268, Train Acc: 0.90667, Test Acc: 0.88235\n",
            "Epoch: 486, Loss: 0.37536, Train Acc: 0.84000, Test Acc: 0.83824\n",
            "Epoch: 487, Loss: 0.44833, Train Acc: 0.91333, Test Acc: 0.88235\n",
            "Epoch: 488, Loss: 0.42839, Train Acc: 0.90667, Test Acc: 0.88235\n",
            "Epoch: 489, Loss: 0.39337, Train Acc: 0.86000, Test Acc: 0.86765\n",
            "Epoch: 490, Loss: 0.39991, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 491, Loss: 0.35239, Train Acc: 0.91333, Test Acc: 0.88235\n",
            "Epoch: 492, Loss: 0.38520, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 493, Loss: 0.34718, Train Acc: 0.91333, Test Acc: 0.83824\n",
            "Epoch: 494, Loss: 0.43081, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 495, Loss: 0.47404, Train Acc: 0.92000, Test Acc: 0.86765\n",
            "Epoch: 496, Loss: 0.72054, Train Acc: 0.88000, Test Acc: 0.83824\n",
            "Epoch: 497, Loss: 0.42523, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 498, Loss: 0.42038, Train Acc: 0.87333, Test Acc: 0.82353\n",
            "Epoch: 499, Loss: 0.45334, Train Acc: 0.88667, Test Acc: 0.82353\n",
            "Epoch: 500, Loss: 0.48161, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 501, Loss: 0.41709, Train Acc: 0.81333, Test Acc: 0.79412\n",
            "Epoch: 502, Loss: 0.41403, Train Acc: 0.78000, Test Acc: 0.79412\n",
            "Epoch: 503, Loss: 0.36284, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 504, Loss: 0.49539, Train Acc: 0.84667, Test Acc: 0.79412\n",
            "Epoch: 505, Loss: 0.37935, Train Acc: 0.87333, Test Acc: 0.85294\n",
            "Epoch: 506, Loss: 0.38617, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 507, Loss: 0.52782, Train Acc: 0.90667, Test Acc: 0.85294\n",
            "Epoch: 508, Loss: 0.47932, Train Acc: 0.89333, Test Acc: 0.85294\n",
            "Epoch: 509, Loss: 0.53830, Train Acc: 0.90667, Test Acc: 0.88235\n",
            "Epoch: 510, Loss: 0.45907, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 511, Loss: 0.41771, Train Acc: 0.88667, Test Acc: 0.83824\n",
            "Epoch: 512, Loss: 0.46466, Train Acc: 0.82667, Test Acc: 0.79412\n",
            "Epoch: 513, Loss: 0.39153, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 514, Loss: 0.44496, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 515, Loss: 0.45233, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 516, Loss: 0.44466, Train Acc: 0.89333, Test Acc: 0.85294\n",
            "Epoch: 517, Loss: 0.49510, Train Acc: 0.84000, Test Acc: 0.79412\n",
            "Epoch: 518, Loss: 0.43553, Train Acc: 0.85333, Test Acc: 0.79412\n",
            "Epoch: 519, Loss: 0.48390, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 520, Loss: 0.41973, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 521, Loss: 0.51541, Train Acc: 0.90667, Test Acc: 0.86765\n",
            "Epoch: 522, Loss: 0.37600, Train Acc: 0.88000, Test Acc: 0.85294\n",
            "Epoch: 523, Loss: 0.38413, Train Acc: 0.90000, Test Acc: 0.86765\n",
            "Epoch: 524, Loss: 0.45242, Train Acc: 0.85333, Test Acc: 0.79412\n",
            "Epoch: 525, Loss: 0.41108, Train Acc: 0.89333, Test Acc: 0.83824\n",
            "Epoch: 526, Loss: 0.38999, Train Acc: 0.78667, Test Acc: 0.79412\n",
            "Epoch: 527, Loss: 0.48403, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 528, Loss: 0.38108, Train Acc: 0.88000, Test Acc: 0.82353\n",
            "Epoch: 529, Loss: 0.36924, Train Acc: 0.83333, Test Acc: 0.83824\n",
            "Epoch: 530, Loss: 0.45891, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 531, Loss: 0.42070, Train Acc: 0.84667, Test Acc: 0.79412\n",
            "Epoch: 532, Loss: 0.58086, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 533, Loss: 0.40651, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 534, Loss: 0.34486, Train Acc: 0.81333, Test Acc: 0.79412\n",
            "Epoch: 535, Loss: 0.43189, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 536, Loss: 0.46555, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 537, Loss: 0.50315, Train Acc: 0.85333, Test Acc: 0.82353\n",
            "Epoch: 538, Loss: 0.36998, Train Acc: 0.80667, Test Acc: 0.76471\n",
            "Epoch: 539, Loss: 0.49821, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 540, Loss: 0.65994, Train Acc: 0.91333, Test Acc: 0.89706\n",
            "Epoch: 541, Loss: 0.39558, Train Acc: 0.92000, Test Acc: 0.89706\n",
            "Epoch: 542, Loss: 0.48531, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 543, Loss: 0.44939, Train Acc: 0.80667, Test Acc: 0.75000\n",
            "Epoch: 544, Loss: 0.42278, Train Acc: 0.77333, Test Acc: 0.79412\n",
            "Epoch: 545, Loss: 0.48069, Train Acc: 0.86000, Test Acc: 0.79412\n",
            "Epoch: 546, Loss: 0.57470, Train Acc: 0.80667, Test Acc: 0.75000\n",
            "Epoch: 547, Loss: 0.50104, Train Acc: 0.88667, Test Acc: 0.82353\n",
            "Epoch: 548, Loss: 0.43869, Train Acc: 0.90667, Test Acc: 0.79412\n",
            "Epoch: 549, Loss: 0.41688, Train Acc: 0.91333, Test Acc: 0.80882\n",
            "Epoch: 550, Loss: 0.39222, Train Acc: 0.86000, Test Acc: 0.83824\n",
            "Epoch: 551, Loss: 0.40206, Train Acc: 0.88667, Test Acc: 0.82353\n",
            "Epoch: 552, Loss: 0.40150, Train Acc: 0.90667, Test Acc: 0.83824\n",
            "Epoch: 553, Loss: 0.41563, Train Acc: 0.91333, Test Acc: 0.86765\n",
            "Epoch: 554, Loss: 0.37905, Train Acc: 0.82667, Test Acc: 0.79412\n",
            "Epoch: 555, Loss: 0.41341, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 556, Loss: 0.42216, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 557, Loss: 0.45805, Train Acc: 0.88667, Test Acc: 0.91176\n",
            "Epoch: 558, Loss: 0.42961, Train Acc: 0.86667, Test Acc: 0.85294\n",
            "Epoch: 559, Loss: 0.41794, Train Acc: 0.92000, Test Acc: 0.89706\n",
            "Epoch: 560, Loss: 0.48811, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 561, Loss: 0.45380, Train Acc: 0.88000, Test Acc: 0.86765\n",
            "Epoch: 562, Loss: 0.40224, Train Acc: 0.93333, Test Acc: 0.89706\n",
            "Epoch: 563, Loss: 0.40604, Train Acc: 0.94000, Test Acc: 0.89706\n",
            "Epoch: 564, Loss: 0.59463, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 565, Loss: 0.46778, Train Acc: 0.89333, Test Acc: 0.86765\n",
            "Epoch: 566, Loss: 0.46863, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 567, Loss: 0.47944, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 568, Loss: 0.42048, Train Acc: 0.91333, Test Acc: 0.89706\n",
            "Epoch: 569, Loss: 0.40098, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 570, Loss: 0.45913, Train Acc: 0.93333, Test Acc: 0.94118\n",
            "Epoch: 571, Loss: 0.49170, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 572, Loss: 0.40487, Train Acc: 0.88667, Test Acc: 0.86765\n",
            "Epoch: 573, Loss: 0.37305, Train Acc: 0.94667, Test Acc: 0.94118\n",
            "Epoch: 574, Loss: 0.37318, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 575, Loss: 0.42246, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 576, Loss: 0.40130, Train Acc: 0.92000, Test Acc: 0.89706\n",
            "Epoch: 577, Loss: 0.43874, Train Acc: 0.93333, Test Acc: 0.89706\n",
            "Epoch: 578, Loss: 0.48176, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 579, Loss: 0.34855, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 580, Loss: 0.44505, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 581, Loss: 0.46923, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 582, Loss: 0.42211, Train Acc: 0.84000, Test Acc: 0.76471\n",
            "Epoch: 583, Loss: 0.47455, Train Acc: 0.84667, Test Acc: 0.83824\n",
            "Epoch: 584, Loss: 0.46301, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 585, Loss: 0.49284, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 586, Loss: 0.40752, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 587, Loss: 0.42854, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 588, Loss: 0.57884, Train Acc: 0.92000, Test Acc: 0.85294\n",
            "Epoch: 589, Loss: 0.44177, Train Acc: 0.91333, Test Acc: 0.92647\n",
            "Epoch: 590, Loss: 0.39591, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 591, Loss: 0.49789, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 592, Loss: 0.42301, Train Acc: 0.86000, Test Acc: 0.85294\n",
            "Epoch: 593, Loss: 0.41462, Train Acc: 0.86000, Test Acc: 0.77941\n",
            "Epoch: 594, Loss: 0.41592, Train Acc: 0.87333, Test Acc: 0.88235\n",
            "Epoch: 595, Loss: 0.42300, Train Acc: 0.90000, Test Acc: 0.92647\n",
            "Epoch: 596, Loss: 0.38534, Train Acc: 0.90000, Test Acc: 0.91176\n",
            "Epoch: 597, Loss: 0.55530, Train Acc: 0.90000, Test Acc: 0.89706\n",
            "Epoch: 598, Loss: 0.30899, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 599, Loss: 0.44881, Train Acc: 0.90000, Test Acc: 0.89706\n",
            "Epoch: 600, Loss: 0.43845, Train Acc: 0.94000, Test Acc: 0.88235\n",
            "Epoch: 601, Loss: 0.53665, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 602, Loss: 0.34645, Train Acc: 0.94667, Test Acc: 0.91176\n",
            "Epoch: 603, Loss: 0.37684, Train Acc: 0.88667, Test Acc: 0.83824\n",
            "Epoch: 604, Loss: 0.34649, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 605, Loss: 0.38362, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 606, Loss: 0.35407, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 607, Loss: 0.35643, Train Acc: 0.92000, Test Acc: 0.88235\n",
            "Epoch: 608, Loss: 0.31957, Train Acc: 0.78667, Test Acc: 0.79412\n",
            "Epoch: 609, Loss: 0.49692, Train Acc: 0.80667, Test Acc: 0.75000\n",
            "Epoch: 610, Loss: 0.42662, Train Acc: 0.90000, Test Acc: 0.86765\n",
            "Epoch: 611, Loss: 0.40512, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 612, Loss: 0.41122, Train Acc: 0.94000, Test Acc: 0.89706\n",
            "Epoch: 613, Loss: 0.31729, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 614, Loss: 0.41529, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 615, Loss: 0.37841, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 616, Loss: 0.35986, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 617, Loss: 0.81323, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 618, Loss: 0.37466, Train Acc: 0.91333, Test Acc: 0.89706\n",
            "Epoch: 619, Loss: 0.39448, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 620, Loss: 0.47877, Train Acc: 0.91333, Test Acc: 0.92647\n",
            "Epoch: 621, Loss: 0.33116, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 622, Loss: 0.46242, Train Acc: 0.93333, Test Acc: 0.88235\n",
            "Epoch: 623, Loss: 0.41061, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 624, Loss: 0.31478, Train Acc: 0.91333, Test Acc: 0.92647\n",
            "Epoch: 625, Loss: 0.46079, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 626, Loss: 0.30439, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 627, Loss: 0.37587, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 628, Loss: 0.39347, Train Acc: 0.90000, Test Acc: 0.91176\n",
            "Epoch: 629, Loss: 0.35470, Train Acc: 0.94000, Test Acc: 0.88235\n",
            "Epoch: 630, Loss: 0.41442, Train Acc: 0.86667, Test Acc: 0.77941\n",
            "Epoch: 631, Loss: 0.46796, Train Acc: 0.77333, Test Acc: 0.76471\n",
            "Epoch: 632, Loss: 0.42218, Train Acc: 0.92000, Test Acc: 0.83824\n",
            "Epoch: 633, Loss: 0.34713, Train Acc: 0.82667, Test Acc: 0.80882\n",
            "Epoch: 634, Loss: 0.56182, Train Acc: 0.92667, Test Acc: 0.88235\n",
            "Epoch: 635, Loss: 0.32867, Train Acc: 0.86667, Test Acc: 0.88235\n",
            "Epoch: 636, Loss: 0.63522, Train Acc: 0.85333, Test Acc: 0.80882\n",
            "Epoch: 637, Loss: 0.40375, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 638, Loss: 0.45481, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 639, Loss: 0.40829, Train Acc: 0.93333, Test Acc: 0.88235\n",
            "Epoch: 640, Loss: 0.42374, Train Acc: 0.92667, Test Acc: 0.86765\n",
            "Epoch: 641, Loss: 0.31079, Train Acc: 0.84667, Test Acc: 0.82353\n",
            "Epoch: 642, Loss: 0.39743, Train Acc: 0.91333, Test Acc: 0.85294\n",
            "Epoch: 643, Loss: 0.39647, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 644, Loss: 0.41826, Train Acc: 0.90667, Test Acc: 0.92647\n",
            "Epoch: 645, Loss: 0.44153, Train Acc: 0.92667, Test Acc: 0.83824\n",
            "Epoch: 646, Loss: 0.41994, Train Acc: 0.85333, Test Acc: 0.86765\n",
            "Epoch: 647, Loss: 0.39044, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 648, Loss: 0.43340, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 649, Loss: 0.36043, Train Acc: 0.91333, Test Acc: 0.94118\n",
            "Epoch: 650, Loss: 0.32751, Train Acc: 0.84000, Test Acc: 0.85294\n",
            "Epoch: 651, Loss: 0.42949, Train Acc: 0.92667, Test Acc: 0.83824\n",
            "Epoch: 652, Loss: 0.38438, Train Acc: 0.80667, Test Acc: 0.80882\n",
            "Epoch: 653, Loss: 0.37558, Train Acc: 0.91333, Test Acc: 0.82353\n",
            "Epoch: 654, Loss: 0.36009, Train Acc: 0.82000, Test Acc: 0.82353\n",
            "Epoch: 655, Loss: 0.40919, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 656, Loss: 0.38555, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 657, Loss: 0.45507, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 658, Loss: 0.42320, Train Acc: 0.88000, Test Acc: 0.79412\n",
            "Epoch: 659, Loss: 0.39148, Train Acc: 0.76667, Test Acc: 0.76471\n",
            "Epoch: 660, Loss: 0.38862, Train Acc: 0.86667, Test Acc: 0.79412\n",
            "Epoch: 661, Loss: 0.37704, Train Acc: 0.84000, Test Acc: 0.82353\n",
            "Epoch: 662, Loss: 0.44576, Train Acc: 0.92000, Test Acc: 0.88235\n",
            "Epoch: 663, Loss: 0.39829, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 664, Loss: 0.38176, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 665, Loss: 0.40012, Train Acc: 0.88667, Test Acc: 0.79412\n",
            "Epoch: 666, Loss: 0.42821, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 667, Loss: 0.38456, Train Acc: 0.92000, Test Acc: 0.89706\n",
            "Epoch: 668, Loss: 0.46712, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 669, Loss: 0.31059, Train Acc: 0.86667, Test Acc: 0.86765\n",
            "Epoch: 670, Loss: 0.47813, Train Acc: 0.91333, Test Acc: 0.83824\n",
            "Epoch: 671, Loss: 0.48793, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 672, Loss: 0.36993, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 673, Loss: 0.36846, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 674, Loss: 0.48230, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 675, Loss: 0.40306, Train Acc: 0.92000, Test Acc: 0.83824\n",
            "Epoch: 676, Loss: 0.33645, Train Acc: 0.87333, Test Acc: 0.88235\n",
            "Epoch: 677, Loss: 0.32246, Train Acc: 0.92000, Test Acc: 0.89706\n",
            "Epoch: 678, Loss: 0.38050, Train Acc: 0.94667, Test Acc: 0.89706\n",
            "Epoch: 679, Loss: 0.38902, Train Acc: 0.92000, Test Acc: 0.88235\n",
            "Epoch: 680, Loss: 0.42392, Train Acc: 0.92000, Test Acc: 0.83824\n",
            "Epoch: 681, Loss: 0.50997, Train Acc: 0.79333, Test Acc: 0.79412\n",
            "Epoch: 682, Loss: 0.40809, Train Acc: 0.84000, Test Acc: 0.76471\n",
            "Epoch: 683, Loss: 0.41278, Train Acc: 0.79333, Test Acc: 0.80882\n",
            "Epoch: 684, Loss: 0.44888, Train Acc: 0.90000, Test Acc: 0.79412\n",
            "Epoch: 685, Loss: 0.29781, Train Acc: 0.86667, Test Acc: 0.82353\n",
            "Epoch: 686, Loss: 0.43198, Train Acc: 0.92000, Test Acc: 0.88235\n",
            "Epoch: 687, Loss: 0.55384, Train Acc: 0.92000, Test Acc: 0.86765\n",
            "Epoch: 688, Loss: 0.37680, Train Acc: 0.82667, Test Acc: 0.82353\n",
            "Epoch: 689, Loss: 0.45400, Train Acc: 0.84667, Test Acc: 0.80882\n",
            "Epoch: 690, Loss: 0.38560, Train Acc: 0.93333, Test Acc: 0.89706\n",
            "Epoch: 691, Loss: 0.36688, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 692, Loss: 0.40311, Train Acc: 0.92000, Test Acc: 0.85294\n",
            "Epoch: 693, Loss: 0.39168, Train Acc: 0.93333, Test Acc: 0.89706\n",
            "Epoch: 694, Loss: 0.38686, Train Acc: 0.92000, Test Acc: 0.85294\n",
            "Epoch: 695, Loss: 0.35675, Train Acc: 0.88667, Test Acc: 0.86765\n",
            "Epoch: 696, Loss: 0.38624, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 697, Loss: 0.33802, Train Acc: 0.91333, Test Acc: 0.83824\n",
            "Epoch: 698, Loss: 0.41914, Train Acc: 0.87333, Test Acc: 0.88235\n",
            "Epoch: 699, Loss: 0.42303, Train Acc: 0.90667, Test Acc: 0.85294\n",
            "Epoch: 700, Loss: 0.48139, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 701, Loss: 0.38451, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 702, Loss: 0.40895, Train Acc: 0.88667, Test Acc: 0.77941\n",
            "Epoch: 703, Loss: 0.37831, Train Acc: 0.91333, Test Acc: 0.92647\n",
            "Epoch: 704, Loss: 0.43960, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 705, Loss: 0.35162, Train Acc: 0.92000, Test Acc: 0.85294\n",
            "Epoch: 706, Loss: 0.37905, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 707, Loss: 0.40698, Train Acc: 0.92667, Test Acc: 0.88235\n",
            "Epoch: 708, Loss: 0.39088, Train Acc: 0.93333, Test Acc: 0.88235\n",
            "Epoch: 709, Loss: 0.41035, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 710, Loss: 0.38515, Train Acc: 0.92000, Test Acc: 0.86765\n",
            "Epoch: 711, Loss: 0.28110, Train Acc: 0.79333, Test Acc: 0.79412\n",
            "Epoch: 712, Loss: 0.45732, Train Acc: 0.92667, Test Acc: 0.88235\n",
            "Epoch: 713, Loss: 0.34785, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 714, Loss: 0.37268, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 715, Loss: 0.34182, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 716, Loss: 0.32835, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 717, Loss: 0.32232, Train Acc: 0.90000, Test Acc: 0.79412\n",
            "Epoch: 718, Loss: 0.36060, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 719, Loss: 0.37725, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 720, Loss: 0.39290, Train Acc: 0.90667, Test Acc: 0.82353\n",
            "Epoch: 721, Loss: 0.33479, Train Acc: 0.80667, Test Acc: 0.77941\n",
            "Epoch: 722, Loss: 0.40431, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 723, Loss: 0.32983, Train Acc: 0.94667, Test Acc: 0.89706\n",
            "Epoch: 724, Loss: 0.29534, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 725, Loss: 0.40305, Train Acc: 0.84667, Test Acc: 0.86765\n",
            "Epoch: 726, Loss: 0.40504, Train Acc: 0.92667, Test Acc: 0.88235\n",
            "Epoch: 727, Loss: 0.43288, Train Acc: 0.92667, Test Acc: 0.88235\n",
            "Epoch: 728, Loss: 0.48361, Train Acc: 0.81333, Test Acc: 0.79412\n",
            "Epoch: 729, Loss: 0.39262, Train Acc: 0.93333, Test Acc: 0.88235\n",
            "Epoch: 730, Loss: 0.34793, Train Acc: 0.89333, Test Acc: 0.86765\n",
            "Epoch: 731, Loss: 0.26352, Train Acc: 0.93333, Test Acc: 0.88235\n",
            "Epoch: 732, Loss: 0.37163, Train Acc: 0.89333, Test Acc: 0.86765\n",
            "Epoch: 733, Loss: 0.39685, Train Acc: 0.94667, Test Acc: 0.91176\n",
            "Epoch: 734, Loss: 0.45535, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 735, Loss: 0.34091, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 736, Loss: 0.34306, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 737, Loss: 0.32067, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 738, Loss: 0.36013, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 739, Loss: 0.35495, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 740, Loss: 0.30796, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 741, Loss: 0.27116, Train Acc: 0.84000, Test Acc: 0.82353\n",
            "Epoch: 742, Loss: 0.42283, Train Acc: 0.95333, Test Acc: 0.91176\n",
            "Epoch: 743, Loss: 0.35212, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 744, Loss: 0.38173, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 745, Loss: 0.41832, Train Acc: 0.92000, Test Acc: 0.88235\n",
            "Epoch: 746, Loss: 0.42219, Train Acc: 0.86000, Test Acc: 0.80882\n",
            "Epoch: 747, Loss: 0.36361, Train Acc: 0.94000, Test Acc: 0.89706\n",
            "Epoch: 748, Loss: 0.48213, Train Acc: 0.93333, Test Acc: 0.88235\n",
            "Epoch: 749, Loss: 0.38327, Train Acc: 0.87333, Test Acc: 0.88235\n",
            "Epoch: 750, Loss: 0.30607, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 751, Loss: 0.42307, Train Acc: 0.93333, Test Acc: 0.89706\n",
            "Epoch: 752, Loss: 0.42574, Train Acc: 0.90667, Test Acc: 0.80882\n",
            "Epoch: 753, Loss: 0.36280, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 754, Loss: 0.53235, Train Acc: 0.83333, Test Acc: 0.76471\n",
            "Epoch: 755, Loss: 0.45403, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 756, Loss: 0.30528, Train Acc: 0.95333, Test Acc: 0.89706\n",
            "Epoch: 757, Loss: 0.39098, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 758, Loss: 0.32718, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 759, Loss: 0.34040, Train Acc: 0.88667, Test Acc: 0.80882\n",
            "Epoch: 760, Loss: 0.38094, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 761, Loss: 0.46597, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 762, Loss: 0.35546, Train Acc: 0.95333, Test Acc: 0.91176\n",
            "Epoch: 763, Loss: 0.33442, Train Acc: 0.87333, Test Acc: 0.83824\n",
            "Epoch: 764, Loss: 0.44879, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 765, Loss: 0.39768, Train Acc: 0.90000, Test Acc: 0.79412\n",
            "Epoch: 766, Loss: 0.31805, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 767, Loss: 0.43425, Train Acc: 0.93333, Test Acc: 0.86765\n",
            "Epoch: 768, Loss: 0.37655, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 769, Loss: 0.40115, Train Acc: 0.84667, Test Acc: 0.80882\n",
            "Epoch: 770, Loss: 0.43431, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 771, Loss: 0.35308, Train Acc: 0.86000, Test Acc: 0.76471\n",
            "Epoch: 772, Loss: 0.44600, Train Acc: 0.84000, Test Acc: 0.80882\n",
            "Epoch: 773, Loss: 0.34180, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 774, Loss: 0.31074, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 775, Loss: 0.34723, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 776, Loss: 0.42811, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 777, Loss: 0.34130, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 778, Loss: 0.36978, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 779, Loss: 0.36667, Train Acc: 0.90667, Test Acc: 0.88235\n",
            "Epoch: 780, Loss: 0.34463, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 781, Loss: 0.34642, Train Acc: 0.89333, Test Acc: 0.91176\n",
            "Epoch: 782, Loss: 0.27828, Train Acc: 0.90667, Test Acc: 0.89706\n",
            "Epoch: 783, Loss: 0.35622, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 784, Loss: 0.33843, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 785, Loss: 0.31692, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 786, Loss: 0.33356, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 787, Loss: 0.30029, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 788, Loss: 0.30578, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 789, Loss: 0.31165, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 790, Loss: 0.52276, Train Acc: 0.91333, Test Acc: 0.82353\n",
            "Epoch: 791, Loss: 0.33733, Train Acc: 0.85333, Test Acc: 0.83824\n",
            "Epoch: 792, Loss: 0.53752, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 793, Loss: 0.39910, Train Acc: 0.90667, Test Acc: 0.79412\n",
            "Epoch: 794, Loss: 0.36163, Train Acc: 0.86000, Test Acc: 0.83824\n",
            "Epoch: 795, Loss: 0.43950, Train Acc: 0.92667, Test Acc: 0.86765\n",
            "Epoch: 796, Loss: 0.43084, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 797, Loss: 0.28702, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 798, Loss: 0.35732, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 799, Loss: 0.60013, Train Acc: 0.86000, Test Acc: 0.88235\n",
            "Epoch: 800, Loss: 0.35563, Train Acc: 0.86667, Test Acc: 0.76471\n",
            "Epoch: 801, Loss: 0.42299, Train Acc: 0.90667, Test Acc: 0.89706\n",
            "Epoch: 802, Loss: 0.34384, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 803, Loss: 0.34752, Train Acc: 0.92000, Test Acc: 0.85294\n",
            "Epoch: 804, Loss: 0.32403, Train Acc: 0.88000, Test Acc: 0.86765\n",
            "Epoch: 805, Loss: 0.29794, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 806, Loss: 0.34844, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 807, Loss: 0.33015, Train Acc: 0.82667, Test Acc: 0.82353\n",
            "Epoch: 808, Loss: 0.32527, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 809, Loss: 0.31525, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 810, Loss: 0.31251, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 811, Loss: 0.32346, Train Acc: 0.91333, Test Acc: 0.89706\n",
            "Epoch: 812, Loss: 0.29185, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 813, Loss: 0.28424, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 814, Loss: 0.32024, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 815, Loss: 0.35309, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 816, Loss: 0.37217, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 817, Loss: 0.36487, Train Acc: 0.96000, Test Acc: 0.94118\n",
            "Epoch: 818, Loss: 0.29823, Train Acc: 0.93333, Test Acc: 0.94118\n",
            "Epoch: 819, Loss: 0.32174, Train Acc: 0.86667, Test Acc: 0.88235\n",
            "Epoch: 820, Loss: 0.26771, Train Acc: 0.96000, Test Acc: 0.94118\n",
            "Epoch: 821, Loss: 0.28731, Train Acc: 0.90667, Test Acc: 0.89706\n",
            "Epoch: 822, Loss: 0.28251, Train Acc: 0.90667, Test Acc: 0.89706\n",
            "Epoch: 823, Loss: 0.26938, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 824, Loss: 0.35965, Train Acc: 0.93333, Test Acc: 0.89706\n",
            "Epoch: 825, Loss: 0.32366, Train Acc: 0.89333, Test Acc: 0.86765\n",
            "Epoch: 826, Loss: 0.33307, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 827, Loss: 0.31637, Train Acc: 0.95333, Test Acc: 0.91176\n",
            "Epoch: 828, Loss: 0.45739, Train Acc: 0.92000, Test Acc: 0.89706\n",
            "Epoch: 829, Loss: 0.36410, Train Acc: 0.83333, Test Acc: 0.80882\n",
            "Epoch: 830, Loss: 0.32167, Train Acc: 0.96000, Test Acc: 0.91176\n",
            "Epoch: 831, Loss: 0.36926, Train Acc: 0.94667, Test Acc: 0.91176\n",
            "Epoch: 832, Loss: 0.40237, Train Acc: 0.80000, Test Acc: 0.79412\n",
            "Epoch: 833, Loss: 0.37482, Train Acc: 0.91333, Test Acc: 0.88235\n",
            "Epoch: 834, Loss: 0.34213, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 835, Loss: 0.38266, Train Acc: 0.84000, Test Acc: 0.82353\n",
            "Epoch: 836, Loss: 0.32005, Train Acc: 0.97333, Test Acc: 0.92647\n",
            "Epoch: 837, Loss: 0.39068, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 838, Loss: 0.34897, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 839, Loss: 0.47991, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 840, Loss: 0.34765, Train Acc: 0.78000, Test Acc: 0.77941\n",
            "Epoch: 841, Loss: 0.47141, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 842, Loss: 0.48062, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 843, Loss: 0.41952, Train Acc: 0.82667, Test Acc: 0.79412\n",
            "Epoch: 844, Loss: 0.38939, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 845, Loss: 0.34028, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 846, Loss: 0.29076, Train Acc: 0.87333, Test Acc: 0.89706\n",
            "Epoch: 847, Loss: 0.48378, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 848, Loss: 0.38003, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 849, Loss: 0.70075, Train Acc: 0.81333, Test Acc: 0.80882\n",
            "Epoch: 850, Loss: 0.37642, Train Acc: 0.85333, Test Acc: 0.86765\n",
            "Epoch: 851, Loss: 0.35173, Train Acc: 0.94667, Test Acc: 0.94118\n",
            "Epoch: 852, Loss: 0.37846, Train Acc: 0.85333, Test Acc: 0.89706\n",
            "Epoch: 853, Loss: 0.30387, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 854, Loss: 0.36230, Train Acc: 0.94667, Test Acc: 0.94118\n",
            "Epoch: 855, Loss: 0.36046, Train Acc: 0.82000, Test Acc: 0.82353\n",
            "Epoch: 856, Loss: 0.36887, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 857, Loss: 0.44553, Train Acc: 0.94000, Test Acc: 0.95588\n",
            "Epoch: 858, Loss: 0.36380, Train Acc: 0.86667, Test Acc: 0.89706\n",
            "Epoch: 859, Loss: 0.32319, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 860, Loss: 0.33388, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 861, Loss: 0.32560, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 862, Loss: 0.35698, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 863, Loss: 0.32180, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 864, Loss: 0.40794, Train Acc: 0.88000, Test Acc: 0.91176\n",
            "Epoch: 865, Loss: 0.33964, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 866, Loss: 0.30692, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 867, Loss: 0.33638, Train Acc: 0.87333, Test Acc: 0.89706\n",
            "Epoch: 868, Loss: 0.39660, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 869, Loss: 0.28547, Train Acc: 0.87333, Test Acc: 0.89706\n",
            "Epoch: 870, Loss: 0.42433, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 871, Loss: 0.31583, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 872, Loss: 0.32004, Train Acc: 0.88667, Test Acc: 0.89706\n",
            "Epoch: 873, Loss: 0.29127, Train Acc: 0.89333, Test Acc: 0.91176\n",
            "Epoch: 874, Loss: 0.32992, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 875, Loss: 0.32490, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 876, Loss: 0.31997, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 877, Loss: 0.37539, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 878, Loss: 0.37292, Train Acc: 0.86667, Test Acc: 0.88235\n",
            "Epoch: 879, Loss: 0.42278, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 880, Loss: 0.32918, Train Acc: 0.86000, Test Acc: 0.88235\n",
            "Epoch: 881, Loss: 0.37231, Train Acc: 0.94667, Test Acc: 0.94118\n",
            "Epoch: 882, Loss: 0.31331, Train Acc: 0.90000, Test Acc: 0.91176\n",
            "Epoch: 883, Loss: 0.29243, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 884, Loss: 0.36646, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 885, Loss: 0.29599, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 886, Loss: 0.35729, Train Acc: 0.83333, Test Acc: 0.85294\n",
            "Epoch: 887, Loss: 0.29421, Train Acc: 0.95333, Test Acc: 0.94118\n",
            "Epoch: 888, Loss: 0.49624, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 889, Loss: 0.27707, Train Acc: 0.82667, Test Acc: 0.83824\n",
            "Epoch: 890, Loss: 0.43074, Train Acc: 0.96000, Test Acc: 0.94118\n",
            "Epoch: 891, Loss: 0.29821, Train Acc: 0.95333, Test Acc: 0.94118\n",
            "Epoch: 892, Loss: 0.33485, Train Acc: 0.82667, Test Acc: 0.85294\n",
            "Epoch: 893, Loss: 0.37262, Train Acc: 0.95333, Test Acc: 0.95588\n",
            "Epoch: 894, Loss: 0.39942, Train Acc: 0.89333, Test Acc: 0.89706\n",
            "Epoch: 895, Loss: 0.28945, Train Acc: 0.86667, Test Acc: 0.89706\n",
            "Epoch: 896, Loss: 0.33602, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 897, Loss: 0.27606, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 898, Loss: 0.29983, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 899, Loss: 0.31326, Train Acc: 0.88667, Test Acc: 0.89706\n",
            "Epoch: 900, Loss: 0.32279, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 901, Loss: 0.32421, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 902, Loss: 0.32814, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 903, Loss: 0.36458, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 904, Loss: 0.31416, Train Acc: 0.88000, Test Acc: 0.89706\n",
            "Epoch: 905, Loss: 0.31446, Train Acc: 0.94000, Test Acc: 0.92647\n",
            "Epoch: 906, Loss: 0.31819, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 907, Loss: 0.30264, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 908, Loss: 0.27524, Train Acc: 0.90000, Test Acc: 0.89706\n",
            "Epoch: 909, Loss: 0.30790, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 910, Loss: 0.39610, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 911, Loss: 0.31306, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 912, Loss: 0.33980, Train Acc: 0.88000, Test Acc: 0.91176\n",
            "Epoch: 913, Loss: 0.33461, Train Acc: 0.88667, Test Acc: 0.89706\n",
            "Epoch: 914, Loss: 0.27704, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 915, Loss: 0.32451, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 916, Loss: 0.32433, Train Acc: 0.86000, Test Acc: 0.85294\n",
            "Epoch: 917, Loss: 0.28158, Train Acc: 0.92667, Test Acc: 0.92647\n",
            "Epoch: 918, Loss: 0.32492, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 919, Loss: 0.31533, Train Acc: 0.86667, Test Acc: 0.85294\n",
            "Epoch: 920, Loss: 0.27373, Train Acc: 0.86667, Test Acc: 0.86765\n",
            "Epoch: 921, Loss: 0.33046, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 922, Loss: 0.31637, Train Acc: 0.88667, Test Acc: 0.89706\n",
            "Epoch: 923, Loss: 0.31206, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 924, Loss: 0.35970, Train Acc: 0.94000, Test Acc: 0.94118\n",
            "Epoch: 925, Loss: 0.28083, Train Acc: 0.90000, Test Acc: 0.91176\n",
            "Epoch: 926, Loss: 0.32167, Train Acc: 0.89333, Test Acc: 0.91176\n",
            "Epoch: 927, Loss: 0.33935, Train Acc: 0.96667, Test Acc: 0.92647\n",
            "Epoch: 928, Loss: 0.30162, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 929, Loss: 0.26560, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 930, Loss: 0.28192, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 931, Loss: 0.30118, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 932, Loss: 0.32742, Train Acc: 0.88667, Test Acc: 0.91176\n",
            "Epoch: 933, Loss: 0.31185, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 934, Loss: 0.35756, Train Acc: 0.92000, Test Acc: 0.92647\n",
            "Epoch: 935, Loss: 0.27568, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 936, Loss: 0.31509, Train Acc: 0.96000, Test Acc: 0.94118\n",
            "Epoch: 937, Loss: 0.27208, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 938, Loss: 0.27932, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 939, Loss: 0.31611, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 940, Loss: 0.26331, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 941, Loss: 0.33500, Train Acc: 0.90000, Test Acc: 0.89706\n",
            "Epoch: 942, Loss: 0.29037, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 943, Loss: 0.25512, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 944, Loss: 0.26261, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 945, Loss: 0.30153, Train Acc: 0.84667, Test Acc: 0.85294\n",
            "Epoch: 946, Loss: 0.32939, Train Acc: 0.96000, Test Acc: 0.92647\n",
            "Epoch: 947, Loss: 0.30653, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 948, Loss: 0.21913, Train Acc: 0.90000, Test Acc: 0.86765\n",
            "Epoch: 949, Loss: 0.28177, Train Acc: 0.94667, Test Acc: 0.91176\n",
            "Epoch: 950, Loss: 0.27971, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 951, Loss: 0.24567, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 952, Loss: 0.34574, Train Acc: 0.90667, Test Acc: 0.88235\n",
            "Epoch: 953, Loss: 0.30160, Train Acc: 0.95333, Test Acc: 0.94118\n",
            "Epoch: 954, Loss: 0.30676, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 955, Loss: 0.25645, Train Acc: 0.88667, Test Acc: 0.88235\n",
            "Epoch: 956, Loss: 0.31221, Train Acc: 0.94000, Test Acc: 0.91176\n",
            "Epoch: 957, Loss: 0.31740, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 958, Loss: 0.26960, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 959, Loss: 0.28252, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 960, Loss: 0.34574, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 961, Loss: 0.29841, Train Acc: 0.94667, Test Acc: 0.92647\n",
            "Epoch: 962, Loss: 0.29436, Train Acc: 0.95333, Test Acc: 0.94118\n",
            "Epoch: 963, Loss: 0.35089, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 964, Loss: 0.30921, Train Acc: 0.92667, Test Acc: 0.89706\n",
            "Epoch: 965, Loss: 0.27861, Train Acc: 0.95333, Test Acc: 0.94118\n",
            "Epoch: 966, Loss: 0.28064, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 967, Loss: 0.24763, Train Acc: 0.90000, Test Acc: 0.89706\n",
            "Epoch: 968, Loss: 0.26705, Train Acc: 0.95333, Test Acc: 0.94118\n",
            "Epoch: 969, Loss: 0.33729, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 970, Loss: 0.29689, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 971, Loss: 0.25125, Train Acc: 0.94667, Test Acc: 0.94118\n",
            "Epoch: 972, Loss: 0.30712, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 973, Loss: 0.28882, Train Acc: 0.90000, Test Acc: 0.88235\n",
            "Epoch: 974, Loss: 0.29177, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 975, Loss: 0.26839, Train Acc: 0.90667, Test Acc: 0.91176\n",
            "Epoch: 976, Loss: 0.29267, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 977, Loss: 0.33057, Train Acc: 0.92000, Test Acc: 0.91176\n",
            "Epoch: 978, Loss: 0.30202, Train Acc: 0.90667, Test Acc: 0.89706\n",
            "Epoch: 979, Loss: 0.29832, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 980, Loss: 0.37844, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 981, Loss: 0.29102, Train Acc: 0.91333, Test Acc: 0.91176\n",
            "Epoch: 982, Loss: 0.27853, Train Acc: 0.93333, Test Acc: 0.91176\n",
            "Epoch: 983, Loss: 0.31086, Train Acc: 0.91333, Test Acc: 0.89706\n",
            "Epoch: 984, Loss: 0.35439, Train Acc: 0.87333, Test Acc: 0.88235\n",
            "Epoch: 985, Loss: 0.28119, Train Acc: 0.93333, Test Acc: 0.92647\n",
            "Epoch: 986, Loss: 0.32468, Train Acc: 0.97333, Test Acc: 0.94118\n",
            "Epoch: 987, Loss: 0.31854, Train Acc: 0.92667, Test Acc: 0.91176\n",
            "Epoch: 988, Loss: 0.24542, Train Acc: 0.89333, Test Acc: 0.88235\n",
            "Epoch: 989, Loss: 0.36500, Train Acc: 0.85333, Test Acc: 0.85294\n",
            "Epoch: 990, Loss: 0.29360, Train Acc: 0.97333, Test Acc: 0.94118\n",
            "Epoch: 991, Loss: 0.30383, Train Acc: 0.97333, Test Acc: 0.94118\n",
            "Epoch: 992, Loss: 0.32312, Train Acc: 0.86000, Test Acc: 0.85294\n",
            "Epoch: 993, Loss: 0.32363, Train Acc: 0.96667, Test Acc: 0.92647\n",
            "Epoch: 994, Loss: 0.27840, Train Acc: 0.95333, Test Acc: 0.92647\n",
            "Epoch: 995, Loss: 0.28502, Train Acc: 0.86000, Test Acc: 0.86765\n",
            "Epoch: 996, Loss: 0.34677, Train Acc: 0.88000, Test Acc: 0.88235\n",
            "Epoch: 997, Loss: 0.30069, Train Acc: 0.97333, Test Acc: 0.92647\n",
            "Epoch: 998, Loss: 0.29084, Train Acc: 0.91333, Test Acc: 0.88235\n",
            "Epoch: 999, Loss: 0.33908, Train Acc: 0.92000, Test Acc: 0.89706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eaOHbbAtHVG5"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}